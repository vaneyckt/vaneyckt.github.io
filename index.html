<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.20.7" />
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <meta property="og:title" content="vaneyckt.io" />
<meta property="og:description" content="" />

<meta property="og:type" content="website" />

<meta property="og:locale" content="en_US" />
<meta property="og:url" content="https://vaneyckt.io/" />


  <title> vaneyckt.io </title>

  
  <link href="https://vaneyckt.io/index.xml" rel="alternate" type="application/rss+xml" title="vaneyckt.io" />
  <link href="https://vaneyckt.io/index.xml" rel="feed" type="application/rss+xml" title="vaneyckt.io" />
  

  <link rel="stylesheet" href="/css/monokai.css">
  <script src="/js/highlight.pack.js"></script>

  <script>hljs.initHighlightingOnLoad();</script>

  
  <link rel="stylesheet" href="https://vaneyckt.io/css/poole.css">
  <link rel="stylesheet" href="https://vaneyckt.io/css/syntax.css">
  <link rel="stylesheet" href="https://vaneyckt.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="https://vaneyckt.io/index.xml" rel="alternate" type="application/rss+xml" title="vaneyckt.io" />

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300' rel='stylesheet' type='text/css'>

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-71853042-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a style="text-decoration:none" href="https://vaneyckt.io">vaneyckt</a></h1>
      <p class="lead">
         notes to my future self 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="https://vaneyckt.io">Home</a></li>
      <li><a href="https://vaneyckt.io/posts">Posts</a></li>
      <li><a href="https://vaneyckt.io/topics">Tags</a></li>
      
      <br/>
      
    </ul>
      
      
      
      <a href="https://github.com/vaneyckt"><i class="fa fa-github-square"></i></a>&nbsp;&nbsp;
      <a href="mailto:tomvaneyck@gmail.com"><i class="fa fa-envelope-square"></i></a>&nbsp;&nbsp;
      <a href="https://vaneyckt.io/index.xml"><i class="fa fa-rss-square"></i></a>&nbsp;&nbsp;
      

    <p class="footnote">powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    &copy; 2016 Tom Van Eyck. All rights reserved.</p>
  </div>
</div>


  <div class="content container">
    <div class="posts">

      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/ruby_concurrency_in_praise_of_condition_variables/">
                Ruby concurrency: in praise of condition variables
              </a>
            </h1>

            <span class="post-date">Jul 17, 2017</span>

            

            

<p>In a previous post, we talked about the benefits conferred by <a href="https://vaneyckt.io/posts/ruby_concurrency_in_praise_of_the_mutex/">Ruby mutexes</a>. While a programmer&rsquo;s familiarity with mutexes is likely to depend on what kind of programs she usually writes, most developers tend to be at least somewhat familiar with these particular synchronization primitives. This article, however, is going to focus on a much lesser-known synchronization construct: the condition variable.</p>

<p>Condition variables are used for putting threads to sleep and waking them back up once a certain condition is met. Don&rsquo;t worry if this sounds a bit vague; we&rsquo;ll go into a lot more detail later. As condition variables always need to be used in conjunction with mutexes, we&rsquo;ll lead with a quick mutex recap. Next, we&rsquo;ll introduce consumer-producer problems and how to elegantly solve them with the aid of condition variables. Then, we&rsquo;ll have a look at how to use these synchronization primitives for implementing blocking method calls. Finishing up, we&rsquo;ll describe some curious condition variable behavior and how to safeguard against it.</p>

<h3 id="a-mutex-recap">A mutex recap</h3>

<p>A mutex is a data structure for protecting shared state between multiple threads. When a piece of code is wrapped inside a mutex, the mutex guarantees that only one thread at a time can execute this code. If another thread wants to start executing this code, it&rsquo;ll have to wait until our first thread is done with it. I realize this may all sound a bit abstract, so now is probably a good time to bring in some example code.</p>

<h4 id="writing-to-shared-state">Writing to shared state</h4>

<p>In this first example, we&rsquo;ll have a look at what happens when two threads try to modify the same shared variable. The snippet below shows two methods: <code>counters_with_mutex</code> and <code>counters_without_mutex</code>. Both methods start by creating a zero-initialized <code>counters</code> array before spawning 5 threads. Each thread will perform 100,000 loops, with every iteration incrementing all elements of the <code>counters</code> array by one. Both methods are the same in every way except for one thing: only one of them uses a mutex.</p>

<pre><code class="language-ruby">def counters_with_mutex
  mutex = Mutex.new
  counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

  5.times.map do
    Thread.new do
      100000.times do
        mutex.synchronize do
          counters.map! { |counter| counter + 1 }
        end
      end
    end
  end.each(&amp;:join)

  counters.inspect
end

def counters_without_mutex
  counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

  5.times.map do
    Thread.new do
      100000.times do
        counters.map! { |counter| counter + 1 }
      end
    end
  end.each(&amp;:join)

  counters.inspect
end

puts counters_with_mutex
# =&gt; [500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000]

puts counters_without_mutex
# =&gt; [500000, 447205, 500000, 500000, 500000, 500000, 203656, 500000, 500000, 500000]
# note that we seem to have lost some increments here due to not using a mutex
</code></pre>

<p>As you can see, only the method that uses a mutex ends up producing the correct result. The method without a mutex seems to have lost some increments. This is because the lack of a mutex makes it possible for our second thread to interrupt our first thread at any point during its execution. This can lead to some serious problems.</p>

<p>For example, imagine that our first thread has just read the first entry of the <code>counters</code> array, incremented it by one, and is now getting ready to write this incremented value back to our array. However, before our first thread can write this incremented value, it gets interrupted by the second thread. This second thread then goes on to read the current value of the first entry, increments it by one, and succeeds in writing the result back to our <code>counters</code> array. Now we have a problem!</p>

<p>We have a problem because the first thread got interrupted before it had a chance to write its incremented value to the array. When the first thread resumes, it will end up overwriting the value that the second thread just placed in the array. This will cause us to essentially lose an increment operation, which explains why our program output has entries in it that are less than 500,000.</p>

<p>All these problems can be avoided by using a mutex. Remember that a thread executing code wrapped by a mutex cannot be interleaved with another thread wanting to execute this same code. Therefore, our second thread would never have gotten interleaved with the first thread, thereby avoiding the possibility of results getting overwritten.</p>

<h4 id="reading-from-shared-state">Reading from shared state</h4>

<p>There&rsquo;s a common misconception that a mutex is only required when writing to a shared variable, and not when reading from it. The snippet below shows 50 threads flipping the boolean values in the <code>flags</code> array over and over again. Many developers think this snippet is without error as the code responsible for changing these values was wrapped inside a mutex. If that were true, then every line of the output of <code>puts flags.to_s</code> should consist of 10 repetitions of either <code>true</code> or <code>false</code>. As we can see below, this is not the case.</p>

<pre><code class="language-ruby">mutex = Mutex.new
flags = [false, false, false, false, false, false, false, false, false, false]

threads = 50.times.map do
  Thread.new do
    100000.times do
      # don't do this! Reading from shared state requires a mutex!
      puts flags.to_s

      mutex.synchronize do
        flags.map! { |f| !f }
      end
    end
  end
end
threads.each(&amp;:join)
</code></pre>

<pre><code class="language-bash">$ ruby flags.rb &gt; output.log
$ grep 'true, false' output.log | wc -l
    30
</code></pre>

<p>What&rsquo;s happening here is that our mutex only guarantees that no two threads can modify the <code>flags</code> array at the same time. However, it is perfectly possible for one thread to start reading from this array while another thread is busy modifying it, thereby causing the first thread to read an array that contains both <code>true</code> and <code>false</code> entries. Luckily, all of this can be easily avoided by wrapping <code>puts flags.to_s</code> inside our mutex. This will guarantee that only one thread at a time can read from or write to the <code>flags</code> array.</p>

<p>Before moving on, I would just like to mention that even very experienced people have gotten tripped up by not using a mutex when accessing shared state. In fact, at one point there even was a <a href="http://www.javaworld.com/article/2074979/java-concurrency/double-checked-locking--clever--but-broken.html">Java design pattern</a> that assumed it was safe to not always use a mutex to do so. Needless to say, this pattern has since been amended.</p>

<h3 id="consumer-producer-problems">Consumer-producer problems</h3>

<p>With that mutex refresher out of the way, we can now start looking at condition variables. Condition variables are best explained by trying to come up with a practical solution to the <a href="https://en.wikipedia.org/wiki/Producer-consumer_problem">consumer-producer problem</a>. In fact, consumer-producer problems are so common that Ruby already has a data structure aimed at solving these: <a href="https://ruby-doc.org/core-2.3.1/Queue.html">the Queue class</a>. This class uses a condition variable to implement the blocking variant of its <code>shift()</code> method. In this article, we made a conscious decision not to use the <code>Queue</code> class. Instead, we&rsquo;re going to write everything from scratch with the help of condition variables.</p>

<p>Let&rsquo;s have a look at the problem that we&rsquo;re going to solve. Imagine that we have a website where users can generate tasks of varying complexity, e.g. a service that allows users to convert uploaded jpg images to pdf. We can think of these users as producers of a steady stream of tasks of random complexity. These tasks will get stored on a backend server that has several worker processes running on it. Each worker process will grab a task, process it, and then grab the next one. These workers are our task consumers.</p>

<p>With what we know about mutexes, it shouldn&rsquo;t be too hard to write a piece of code that mimics the above scenario. It&rsquo;ll probably end up looking something like this.</p>

<pre><code class="language-ruby">tasks = []
mutex = Mutex.new
threads = []

class Task
  def initialize
    @duration = rand()
  end

  def execute
    sleep @duration
  end
end

# producer threads
threads += 2.times.map do
  Thread.new do
    while true
      mutex.synchronize do
        tasks &lt;&lt; Task.new
        puts &quot;Added task: #{tasks.last.inspect}&quot;
      end
      # limit task production speed
      sleep 0.5
    end
  end
end

# consumer threads
threads += 5.times.map do
  Thread.new do
    while true
      task = nil
      mutex.synchronize do
        if tasks.count &gt; 0
          task = tasks.shift
          puts &quot;Removed task: #{task.inspect}&quot;
        end
      end
      # execute task outside of mutex so we don't unnecessarily
      # block other consumer threads
      task.execute unless task.nil?
    end
  end
end

threads.each(&amp;:join)
</code></pre>

<p>The above code should be fairly straightforward. There is a <code>Task</code> class for creating tasks that take between 0 and 1 seconds to run. We have 2 producer threads, each running an endless <code>while</code> loop that safely appends a new task to the <code>tasks</code> array every 0.5 seconds with the help of a mutex. Our 5 consumer threads are also running an endless <code>while</code> loop, each iteration grabbing the mutex so as to safely check the <code>tasks</code> array for available tasks. If a consumer thread finds an available task, it removes the task from the array and starts processing it. Once the task had been processed, the thread moves on to its next iteration, thereby repeating the cycle anew.</p>

<p>While the above implementation seems to work just fine, it is not optimal as it requires all consumer threads to constantly poll the <code>tasks</code> array for available work. This polling does not come for free. The Ruby interpreter has to constantly schedule the consumer threads to run, thereby preempting threads that may have actual important work to do. To give an example, the above code will interleave consumer threads that are executing a task with consumer threads that just want to check for newly available tasks. This can become a real problem when there is a large number of consumer threads and only a few tasks.</p>

<p>If you want to see for yourself just how inefficient this approach is, you only need to modify the original code for consumer threads with the code shown below. This modified program prints well over a thousand lines of <code>This thread has nothing to do</code> for every single line of <code>Removed task</code>. Hopefully, this gives you an indication of the general wastefulness of having consumer threads constantly poll the <code>tasks</code> array.</p>

<pre><code class="language-ruby"># modified consumer threads code
threads += 5.times.map do
  Thread.new do
    while true
      task = nil
      mutex.synchronize do
        if tasks.count &gt; 0
          task = tasks.shift
          puts &quot;Removed task: #{task.inspect}&quot;
        else
          puts 'This thread has nothing to do'
        end
      end
      # execute task outside of mutex so we don't unnecessarily
      # block other consumer threads
      task.execute unless task.nil?
    end
  end
end
</code></pre>

<h3 id="condition-variables-to-the-rescue">Condition variables to the rescue</h3>

<p>So how we can create a more efficient solution to the consumer-producer problem? That is where condition variables come into play. Condition variables are used for putting threads to sleep and waking them only once a certain condition is met. Remember that our current solution to the producer-consumer problem is far from ideal because consumer threads need to constantly poll for new tasks to arrive. Things would be much more efficient if our consumer threads could go to sleep and be woken up only when a new task has arrived.</p>

<p>Shown below is a solution to the consumer-producer problem that makes use of condition variables. We&rsquo;ll talk about how this works in a second. For now though, just have a look at the code and perhaps have a go at running it. If you were to run it, you would probably see that <code>This thread has nothing to do</code> does not show up anymore. Our new approach has completely gotten rid of consumer threads busy polling the <code>tasks</code> array.</p>

<p>The use of a condition variable will now cause our consumer threads to wait for a task to be available in the <code>tasks</code> array before proceeding. As a result of this, we can now remove some of the checks we had to have in place in our original consumer code. I&rsquo;ve added some comments to the code below to help highlight these removals.</p>

<pre><code class="language-ruby">tasks = []
mutex = Mutex.new
cond_var = ConditionVariable.new
threads = []

class Task
  def initialize
    @duration = rand()
  end

  def execute
    sleep @duration
  end
end

# producer threads
threads += 2.times.map do
  Thread.new do
    while true
      mutex.synchronize do
        tasks &lt;&lt; Task.new
        cond_var.signal
        puts &quot;Added task: #{tasks.last.inspect}&quot;
      end
      # limit task production speed
      sleep 0.5
    end
  end
end

# consumer threads
threads += 5.times.map do
  Thread.new do
    while true
      task = nil
      mutex.synchronize do
        while tasks.empty?
          cond_var.wait(mutex)
        end

        # the `if tasks.count == 0` statement will never be true as the thread
        # will now only reach this line if the tasks array is not empty
        puts 'This thread has nothing to do' if tasks.count == 0

        # similarly, we can now remove the `if tasks.count &gt; 0` check that
        # used to surround this code. We no longer need it as this code will
        # now only get executed if the tasks array is not empty.
        task = tasks.shift
        puts &quot;Removed task: #{task.inspect}&quot;
      end
      # Note that we have now removed `unless task.nil?` from this line as
      # our thread can only arrive here if there is indeed a task available.
      task.execute
    end
  end
end

threads.each(&amp;:join)
</code></pre>

<p>Aside from us removing some <code>if</code> statements, our new code is essentially identical to our previous solution. The only exception to this are the five new lines shown below. Don&rsquo;t worry if some of the accompanying comments don&rsquo;t quite make sense yet. Now is also a good time to point out that the new code for both the producer and consumer threads was added inside the existing mutex synchronization blocks. Condition variables are not thread-safe and therefore always need to be used in conjunction with a mutex!</p>

<pre><code class="language-ruby"># declaring the condition variable
cond_var = ConditionVariable.new
</code></pre>

<pre><code class="language-ruby"># a producer thread now signals the condition variable
# after adding a new task to the tasks array
cond_var.signal
</code></pre>

<pre><code class="language-ruby"># a consumer thread now goes to sleep when it sees that
# the tasks array is empty. It can get woken up again
# when a producer thread signals the condition variable.
while tasks.empty?
  cond_var.wait(mutex)
end
</code></pre>

<p>Let&rsquo;s talk about the new code now. We&rsquo;ll start with the consumer threads snippet. There&rsquo;s actually so much going on in these three lines that we&rsquo;ll limit ourselves to covering what <code>cond_var.wait(mutex)</code> does for now. We&rsquo;ll explain the need for the <code>while tasks.empty?</code> loop later. The first thing to notice about the <code>wait</code> method is the parameter that&rsquo;s being passed to it. Remember how a condition variable is not thread-safe and therefore should only have its methods called inside a mutex synchronization block? It is that mutex that needs to be passed as a parameter to the <code>wait</code> method.</p>

<p>Calling <code>wait</code> on a condition variable causes two things to happen. First of all, it causes the thread that calls <code>wait</code> to go to sleep. That is to say, the thread will tell the interpreter that it no longer wants to be scheduled. However, this thread still has ownership of the mutex as it&rsquo;s going to sleep. We need to ensure that the thread relinquishes this mutex because otherwise all other threads waiting for this mutex will be blocked. By passing this mutex to the <code>wait</code> method, the <code>wait</code> method internals will ensure that the mutex gets released as the thread goes to sleep.</p>

<p>Let&rsquo;s move on to the producer threads. These threads are now calling <code>cond_var.signal</code>. The <code>signal</code> method is pretty straightforward in that it wakes up exactly one of the threads that were put to sleep by the <code>wait</code> method. This newly awoken thread will indicate to the interpreter that it is ready to start getting scheduled again and then wait for its turn.</p>

<p>So what code does our newly awoken thread start executing once it gets scheduled again? It starts executing from where it left off. Essentially, a newly awoken thread will return from its call to <code>cond_var.wait(mutex)</code> and resume from there. Personally, I like to think of calling <code>wait</code> as creating a save point inside a thread from which work can resume once the thread gets woken up and rescheduled again. Please note that since the thread wants to resume from where it originally left off, it&rsquo;ll need to reacquire the mutex in order to get scheduled. This mutex reacquisition is very important, so be sure to remember it.</p>

<p>This segues nicely into why we need to use <code>while tasks.empty?</code> when calling <code>wait</code> in a consumer thread. When our newly awoken thread resumes execution by returning from <code>cond_var.wait</code>, the first thing it&rsquo;ll do is complete its previously interrupted iteration through the <code>while</code> loop, thereby evaluating <code>while tasks.empty?</code> again. This actually causes us to neatly avoid a possible race condition.</p>

<p>Let&rsquo;s say we don&rsquo;t use a <code>while</code> loop and use an <code>if</code> statement instead. The resulting code would then look like shown below. Unfortunately, there is a very hard to find problem with this code. Note how we now need to re-add the previously removed <code>if tasks.count &gt; 0</code> and <code>unless task.nil?</code> statements to our code below in order to ensure its safe execution.</p>

<pre><code class="language-ruby"># consumer threads
threads += 5.times.map do
  Thread.new do
    while true
      task = nil
      mutex.synchronize do
        cond_var.wait(mutex) if tasks.empty?

        # using `if tasks.empty?` forces us to once again add this
        # `if tasks.count &gt; 0` check. We need this check to protect
        # ourselves against a nasty race condition.
        if tasks.count &gt; 0
          task = tasks.shift
          puts &quot;Removed task: #{task.inspect}&quot;
        else
          puts 'This thread has nothing to do'
        end
      end
      # using `if tasks.empty?` forces us to re-add `unless task.nil?`
      # in order to safeguard ourselves against a now newly introduced
      # race condition
      task.execute unless task.nil?
    end
  end
end
</code></pre>

<p>Imagine a scenario where we have:</p>

<ul>
<li>two producer threads</li>
<li>one consumer thread that&rsquo;s awake</li>
<li>four consumer threads that are asleep</li>
</ul>

<p>A consumer thread that&rsquo;s awake will go back to sleep only when there are no more tasks in the <code>tasks</code> array. That is to say, a single consumer thread will keep processing tasks until no more tasks are available. Now, let&rsquo;s say one of our producer threads adds a new task to the currently empty <code>tasks</code> array before calling <code>cond_var.signal</code> at roughly the same time as our active consumer thread is finishing its current task. This <code>signal</code> call will awaken one of our sleeping consumer threads, which will then try to get itself scheduled. This is where a race condition is likely to happen!</p>

<p>We&rsquo;re now in a position where two consumer threads are competing for ownership of the mutex in order to get scheduled. Let&rsquo;s say our first consumer thread wins this competition. This thread will now go and grab the task from the <code>tasks</code> array before relinquishing the mutex. Our second consumer thread then grabs the mutex and gets to run. However, as the <code>tasks</code> array is empty now, there is nothing for this second consumer thread to work on. So this second consumer thread now has to do an entire iteration of its <code>while true</code> loop for no real purpose at all.</p>

<p>We now find ourselves in a situation where a complete iteration of the <code>while true</code> loop can occur even when the <code>tasks</code> array is empty. This is a not unlike the position we were in when our program was just busy polling the <code>tasks</code> array. Sure, our current program will be more efficient than busy polling, but we will still need to safeguard our code against the possibility of an iteration occurring when there is no task available. This is why we needed to re-add the <code>if tasks.count &gt; 0</code> and <code>unless task.nil?</code> statements. Especially the latter of these two is important, as otherwise our program might crash with a <code>NilException</code>.</p>

<p>Luckily, we can safely get rid of these easily overlooked safeguards by forcing each newly awakened consumer thread to check for available tasks and having it put itself to sleep again if no tasks are available. This behavior can be accomplished by replacing the <code>if tasks.empty?</code> statement with a <code>while tasks.empty?</code> loop. If tasks are available, a newly awoken thread will exit the loop and execute the rest of its code. However, if no tasks are found, then the loop is repeated, thereby causing the thread to put itself to sleep again by executing <code>cond_var.wait</code>. We&rsquo;ll see in a later section that there is yet another benefit to using this <code>while</code> loop.</p>

<h3 id="building-our-own-queue-class">Building our own Queue class</h3>

<p>At the beginning of a previous section, we touched on how condition variables are used by the <code>Queue</code> class to implement blocking behavior. The previous section taught us enough about condition variables for us to go and implement a basic <code>Queue</code> class ourselves. We&rsquo;re going to create a thread-safe <code>SimpleQueue</code> class that is capable of:</p>

<ul>
<li>having data appended to it with the <code>&lt;&lt;</code> operator</li>
<li>having data retrieved from it with a non-blocking <code>shift</code> method</li>
<li>having data retrieved from it with a blocking <code>shift</code> method</li>
</ul>

<p>It&rsquo;s easy enough to write code that meets these first two criteria. It will probably end up looking something like the code shown below. Note that our <code>SimpleQueue</code> class is using a mutex as we want this class to be thread-safe, just like the original <code>Queue</code> class.</p>

<pre><code class="language-ruby">class SimpleQueue
  def initialize
    @elems = []
    @mutex = Mutex.new
  end

  def &lt;&lt;(elem)
    @mutex.synchronize do
      @elems &lt;&lt; elem
    end
  end

  def shift(blocking = true)
    @mutex.synchronize do
      if blocking
        raise 'yet to be implemented'
      end
      @elems.shift
    end
  end
end

simple_queue = SimpleQueue.new
simple_queue &lt;&lt; 'foo'

simple_queue.shift(false)
# =&gt; &quot;foo&quot;

simple_queue.shift(false)
# =&gt; nil
</code></pre>

<p>Now let&rsquo;s have a look at what&rsquo;s needed to implement the blocking <code>shift</code> behavior. As it turns out, this is actually very easy. We only want the thread to block if the <code>shift</code> method is called when the <code>@elems</code> array is empty. This is all the information we need to determine where we need to place our condition variable&rsquo;s call to <code>wait</code>. Similarly, we want the thread to stop blocking once the <code>&lt;&lt;</code> operator appends a new element, thereby causing <code>@elems</code> to no longer be empty. This tells us exactly where we need to place our call to <code>signal</code>.</p>

<p>In the end, we just need to create a condition variable that makes the thread go to sleep when a blocking <code>shift</code> is called on an empty <code>SimpleQueue</code>. Likewise, the <code>&lt;&lt;</code> operator just needs to signal the condition variable when a new element is added, thereby causing the sleeping thread to be woken up. The takeaway from this is that blocking methods work by causing their calling thread to fall asleep. Also, please note that the call to <code>@cond_var.wait</code> takes place inside a <code>while @elems.empty?</code> loop. Always use a <code>while</code> loop when calling <code>wait</code> on a condition variable! Never use an <code>if</code> statement!</p>

<pre><code class="language-ruby">class SimpleQueue
  def initialize
    @elems = []
    @mutex = Mutex.new
    @cond_var = ConditionVariable.new
  end

  def &lt;&lt;(elem)
    @mutex.synchronize do
      @elems &lt;&lt; elem
      @cond_var.signal
    end
  end

  def shift(blocking = true)
    @mutex.synchronize do
      if blocking
        while @elems.empty?
          @cond_var.wait(@mutex)
        end
      end
      @elems.shift
    end
  end
end

simple_queue = SimpleQueue.new

# this will print &quot;blocking shift returned with: foo&quot; after 5 seconds
# that is to say, the first thread will go to sleep until the second
# thread adds an element to the queue, thereby causing the first thread
# to be woken up again
threads = []
threads &lt;&lt; Thread.new { puts &quot;blocking shift returned with: #{simple_queue.shift}&quot; }
threads &lt;&lt; Thread.new { sleep 5; simple_queue &lt;&lt; 'foo' }
threads.each(&amp;:join)
</code></pre>

<p>One thing to point out in the above code is that <code>@cond_var.signal</code> can get called even when there are no sleeping threads around. This is a perfectly okay thing to do. In these types of scenarios calling <code>@cond_var.signal</code> will just do nothing.</p>

<h3 id="spurious-wakeups">Spurious wakeups</h3>

<p>A &ldquo;spurious wakeup&rdquo; refers to a sleeping thread getting woken up without any <code>signal</code> call having been made. This is an impossible to avoid edge-case in condition variables. It&rsquo;s important to point out that this is not being caused by a bug in the Ruby interpreter or anything like that. Instead, the designers of the threading libraries used by your OS found that allowing for the occasional spurious wakeup <a href="https://stackoverflow.com/a/8594644/1420382">greatly improves the speed of condition variable operations</a>. As such, any code that uses condition variables needs to take spurious wakeups into account.</p>

<p>So does this mean that we need to rewrite all the code that we&rsquo;ve written in this article in an attempt to make it resistant to possible bugs introduced by spurious wakeups? You&rsquo;ll be glad to know that this isn&rsquo;t the case as all code snippets in this article have always wrapped the <code>cond_var.wait</code> statement inside a <code>while</code> loop!</p>

<p>We covered earlier how using a <code>while</code> loop makes our code more efficient when dealing with certain race conditions as it causes a newly awakened thread to check whether there is actually anything to do for it, and if not, the thread goes back to sleep. This same <code>while</code> loop helps us deal with spurious wakeups as well.</p>

<p>When a thread gets woken up by a spurious wakeup and there is nothing for it to do, our usage of a <code>while</code> loop will cause the thread to detect this and go back to sleep. From the thread&rsquo;s point of view, being awakened by a spurious wakeup isn&rsquo;t any different than being woken up with no available tasks to do. So the same mechanism that helps us deal with race conditions solves our spurious wakeup problem as well. It should be obvious by now that <code>while</code> loops play a very important role when working with condition variables.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Ruby&rsquo;s condition variables are somewhat notorious for their poor documentation. That&rsquo;s a shame, because they are wonderful data structures for efficiently solving a very specific set of problems. Although, as we&rsquo;ve seen, using them isn&rsquo;t without pitfalls. I hope that this post will go some way towards making them (and their pitfalls) a bit better understood in the wider Ruby community.</p>

<p>I also feel like I should point out that while everything mentioned above is correct to the best of my knowledge, I&rsquo;m unable to guarantee that absolutely no mistakes snuck in while writing this. As always, please feel free to contact me if you think I got anything wrong, or even if you just want to say hello.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/using_dtrace_to_measure_mutex_contention_in_ruby/">
                Using DTrace to measure mutex contention in Ruby
              </a>
            </h1>

            <span class="post-date">Oct 31, 2016</span>

            

            

<p>I recently found myself working on Ruby code containing a sizable number of threads and mutexes. It wasn&rsquo;t before long that I started wishing for some tool that could tell me which particular mutexes were the most heavily contended. After all, this type of information can be worth its weight in gold when trying to diagnose why your threaded program is running slower than expected.</p>

<p>This is where <a href="http://dtrace.org/guide/preface.html">DTrace</a> enters the picture. DTrace is a tracing framework that enables you to instrument application and system internals, thereby allowing you to measure and collect previously inaccessible metrics. Personally, I think of DTrace as black magic that allows me to gather a ridiculous amount of information about what is happening on my computer. We will see later just how fine-grained this information can get.</p>

<p>DTrace is available on Solaris, OS X, and FreeBSD. There is some Linux support as well, but you might be better off using one of the Linux specific alternatives instead. Info about these can be found <a href="http://www.brendangregg.com/dtrace.html">here</a> and <a href="http://www.brendangregg.com/blog/2015-07-08/choosing-a-linux-tracer.html">here</a>. At the time of writing this, it was also recently announced that <a href="http://www.brendangregg.com/blog/2016-10-27/dtrace-for-linux-2016.html">DTrace is making its way to Linux</a>. Please note that all work for this particular post was done on a MacBook running OS X El Capitan (version 10.11).</p>

<h3 id="enabling-dtrace-for-ruby-on-os-x">Enabling DTrace for Ruby on OS X</h3>

<p>El Capitan comes with a new security feature called <a href="https://en.wikipedia.org/wiki/System_Integrity_Protection">System Integrity Protection</a> that helps prevent malicious software from tampering with your system. Regrettably, it also prevents DTrace from working. We will need to disable SIP by following <a href="http://stackoverflow.com/a/33584192/1420382">these instructions</a>. Note that it is possible to only partially disable SIP, although doing so will still leave DTrace unable to attach to restricted processes. Personally, I&rsquo;ve completely disabled SIP on my machine.</p>

<p>Next, we want to get DTrace working with Ruby. Although Ruby has DTrace support, there is a very good chance that your currently installed Ruby binary does not have this support enabled. This is especially likely to be true if you compiled your Ruby binary locally on an OS X system, as OS X does not allow the Ruby compilation process to access the system DTrace binary, thereby causing the resulting Ruby binary to lack DTrace functionality. More information about this can be found <a href="http://stackoverflow.com/a/29232051/1420382">here</a>.</p>

<p>I&rsquo;ve found the easiest way to get a DTrace compatible Ruby on my system was to just go and download a precompiled Ruby binary. Naturally, we want to be a bit careful about this, as downloading random binaries from the internet is not the safest thing. Luckily, the good people over at <a href="https://rvm.io/">rvm.io</a> host DTrace compatible Ruby binaries that we can safely download.</p>

<pre><code class="language-bash">$ rvm mount -r https://rvm.io/binaries/osx/10.10/x86_64/ruby-2.2.3.tar.bz2
$ rvm use ruby-2.2.3
</code></pre>

<p>If you do not wish to install rvm, and cannot find a DTrace compatible Ruby through your favorite version manager, then you could do something similar to the snippet shown below.</p>

<pre><code class="language-bash">$ wget https://rvm.io/binaries/osx/10.11/x86_64/ruby-2.2.3.tar.bz2
$ bunzip2 ruby-2.2.3.tar.bz2
$ tar xf ruby-2.2.3.tar
$ mv ruby-2.2.3/bin/ruby /usr/local/bin/druby
$ rm -r ruby-2.2.3 ruby-2.2.3.tar
</code></pre>

<p>Running the above snippet will install a DTrace compatible Ruby on our system. Note that we rename the binary to <code>druby</code> so as to prevent conflicts with existing Ruby installations. The above approach should really be treated as a last resort. I strongly urge you to make the effort to find a DTrace compatible Ruby binary through your current version manager.</p>

<p>Now that we&rsquo;ve ensured we&rsquo;ll be able to use DTrace with our installed Ruby, let&rsquo;s move on and start learning some DTrace basics.</p>

<h3 id="dtrace-basics">DTrace basics</h3>

<p>DTrace is all about probes. A probe is a piece of code that fires when a specific condition is met. For example, we could ask DTrace to instrument a process with a probe that activates when the process returns from a particular system call. Such a probe would also be able to inspect the value returned by the system call made by this process.</p>

<p>You interact with DTrace by writing scripts in the D scripting language (not related to the D programming language). This language is a mix of C and awk, and has a very low learning curve. Below you can see an example of such a script written in D. This particular script will list all system calls being initiated on my machine along with the name of the process that initiated them. We will save this file as <code>syscall_entry.d</code>.</p>

<pre><code class="language-C">/* syscall_entry.d */
syscall:*:*:entry
{
  printf(&quot;\tprocess_name: %s&quot;, execname);
}
</code></pre>

<p>The first line of our script tells DTrace which probes we want to use. In this particular case, we are using <code>syscall:*:*:entry</code> to match every single probe associated with initiating a system call. DTrace has individual probes for every possible system call, so if DTrace were to have no built-in functionality for matching multiple probes, I would have been forced to manually specify every single system call probe myself, and our script would have been a whole lot longer.</p>

<p>I want to briefly cover some DTrace terminology before continuing on. Every DTrace probe adheres to the <code>&lt;provider&gt;:&lt;module&gt;:&lt;function&gt;:&lt;name&gt;</code> description format. In the script above we asked DTrace to match all probes of the <code>syscall</code> provider that have <code>entry</code> as their name. In this particular example, we explicitly used the <code>*</code> character to show that we want to match multiple probes. However, keep in mind that the use of the <code>*</code> character is optional. Most DTrace documentation would opt for <code>syscall:::entry</code> instead.</p>

<p>The rest of the script is rather straightforward. We are basically just telling DTrace to print the <code>execname</code> every time a probe fires. The <code>execname</code> is a built-in DTrace variable that holds the name of the process that caused the probe to be fired. Let us now go ahead and run our simple DTrace script.</p>

<pre><code class="language-bash">$ sudo dtrace -s syscall_entry.d

dtrace: script 'syscall_entry.d' matched 500 probes
CPU     ID                    FUNCTION:NAME
  0    249                      ioctl:entry    process_name: dtrace
  0    373               gettimeofday:entry    process_name: java
  0    249                      ioctl:entry    process_name: dtrace
  0    751              psynch_cvwait:entry    process_name: java
  0    545                     sysctl:entry    process_name: dtrace
  0    545                     sysctl:entry    process_name: dtrace
  0    233                  sigaction:entry    process_name: dtrace
  0    233                  sigaction:entry    process_name: dtrace
  0    751              psynch_cvwait:entry    process_name: dtrace
  0    889                 kevent_qos:entry    process_name: Google Chrome Helper
  0    889                 kevent_qos:entry    process_name: Google Chrome Helper
  0    877           workq_kernreturn:entry    process_name: notifyd
  ...
</code></pre>

<p>The first thing to notice is that <code>syscall:*:*:entry</code> matched 500 different probes. At first glance this might seem like a lot, but on my machine there are well over 330,000 probes available. You can list all DTrace probes on your machine by running <code>sudo dtrace -l</code>.</p>

<p>The second thing to notice is the insane amount of data returned by DTrace. The snippet above really doesn&rsquo;t do the many hundreds of lines of output justice, but going forward we&rsquo;ll see how we can get DTrace to output just those things we are interested in.</p>

<p>Before moving on to the next section, I just want to note that the D scripting language is not Turing complete. It lacks such features as conditional branching and loops. DTrace is built around the ideas of minimal overhead and absolute safety. Giving people the ability to use DTrace to introduce arbitrary overhead on top of system calls does not fit with these ideas.</p>

<h3 id="ruby-and-dtrace">Ruby and DTrace</h3>

<p>DTrace probes have been supported by Ruby <a href="https://tenderlovemaking.com/2011/12/05/profiling-rails-startup-with-dtrace.html">since Ruby 2.0 came out</a>. A list of supported Ruby probes can be found <a href="http://ruby-doc.org/core-2.2.3/doc/dtrace_probes_rdoc.html">here</a>. Now is a good time to mention that DTrace probes come in two flavors: dynamic probes and static probes. Dynamic probes only appear in the <code>pid</code> and <code>fbt</code> probe providers. This means that the vast majority of available probes (including Ruby probes) is static.</p>

<p>So how exactly do dynamic and static probes differ? In order to explain this, we first need to take a closer look at just how DTrace works. When you invoke DTrace on a process you are effectively giving DTrace permission to patch additional DTrace instrumentation instructions into the process&rsquo;s address space. Remember how we had to disable the System Integrity Protection check in order to get DTrace to work on El Capitan? This is why.</p>

<p>In the case of dynamic probes, DTrace instrumentation instructions only get patched into a process when DTrace is invoked on this process. In other words, dynamic probes add zero overhead when not enabled. Static probes on the other hand have to be compiled into the binary that wants to make use of them. This is done through a <a href="https://github.com/ruby/ruby/blob/trunk/probes.d">probes.d file</a>.</p>

<p>However, even when probes have been compiled into the binary, this does not necessarily mean that they are getting triggered. When a process with static probes in its binary does not have DTrace invoked on it, any probe instructions get converted into NOP operations.
This usually introduces a negligible, but nevertheless non-zero, performance impact. More information about DTrace overhead can be found <a href="http://dtrace.org/blogs/brendan/2011/02/18/dtrace-pid-provider-overhead/">here</a>, <a href="http://www.solarisinternals.com/wiki/index.php/DTrace_Topics_Overhead#Dynamic_Probes">here</a>, and <a href="http://www.solarisinternals.com/wiki/index.php/DTrace_Topics_Overhead#Static_Probes">here</a>.</p>

<p>Now that we&rsquo;ve immersed ourselves in all things probe-related, let&rsquo;s go ahead and actually list which DTrace probes are available for a Ruby process. We saw earlier that Ruby comes with static probes compiled into the Ruby binary. We can ask DTrace to list these probes for us with the following command.</p>

<pre><code class="language-bash">$ sudo dtrace -l -m ruby -c 'ruby -v'

ID         PROVIDER       MODULE              FUNCTION      NAME
114188    ruby86029         ruby       empty_ary_alloc      array-create
114189    ruby86029         ruby               ary_new      array-create
114190    ruby86029         ruby         vm_call_cfunc      cmethod-entry
114191    ruby86029         ruby         vm_call0_body      cmethod-entry
114192    ruby86029         ruby          vm_exec_core      cmethod-entry
114193    ruby86029         ruby         vm_call_cfunc      cmethod-return
114194    ruby86029         ruby         vm_call0_body      cmethod-return
114195    ruby86029         ruby            rb_iterate      cmethod-return
114196    ruby86029         ruby          vm_exec_core      cmethod-return
114197    ruby86029         ruby       rb_require_safe      find-require-entry
114198    ruby86029         ruby       rb_require_safe      find-require-return
114199    ruby86029         ruby              gc_marks      gc-mark-begin
...
</code></pre>

<p>Let&rsquo;s take a moment to look at the command we entered here. We saw earlier that we can use <code>-l</code> to have DTrace list its probes. Now we also use <code>-m ruby</code> to get DTrace to limit its listing to probes from the ruby module. However, DTrace will only list its Ruby probes if you specifically tell it you are interested in invoking DTrace on a Ruby process. This is what we use <code>-c 'ruby -v'</code> for. The <code>-c</code> parameter allows us to specify a command that creates a process we want to run DTrace against. Here we are using <code>ruby -v</code> to spawn a small Ruby process in order to get DTrace to list its Ruby probes.</p>

<p>The above snippet doesn&rsquo;t actually list all Ruby probes, as the <code>sudo dtrace -l</code> command will omit any probes from the pid provider. This is because the pid provider actually defines a <a href="http://dtrace.org/guide/chp-pid.html">class of providers</a>, each of which gets its own set of probes depending on which process you are tracing. Each pid probe corresponds to an internal C function that can be called by that particular process. Below we show how to list the Ruby specific probes of this provider.</p>

<pre><code class="language-bash">$ sudo dtrace -l -n 'pid$target:::entry' -c 'ruby -v' | grep 'ruby'

ID         PROVIDER       MODULE                     FUNCTION      NAME
1395302    pid86272         ruby                   rb_ary_eql      entry
1395303    pid86272         ruby                  rb_ary_hash      entry
1395304    pid86272         ruby                  rb_ary_aset      entry
1395305    pid86272         ruby                    rb_ary_at      entry
1395306    pid86272         ruby                 rb_ary_fetch      entry
1395307    pid86272         ruby                 rb_ary_first      entry
1395308    pid86272         ruby                rb_ary_push_m      entry
1395309    pid86272         ruby                 rb_ary_pop_m      entry
1395310    pid86272         ruby               rb_ary_shift_m      entry
1395311    pid86272         ruby                rb_ary_insert      entry
1395312    pid86272         ruby            rb_ary_each_index      entry
1395313    pid86272         ruby          rb_ary_reverse_each      entry
...
</code></pre>

<p>Here we are only listing the pid entry probes, but keep in mind that every entry probe has a corresponding pid return probe. These probes are great as they provide us with insight into which internal functions are getting called, the arguments passed to these, as well as their return values, and even the offset in the function of the return instruction (useful for when a function has multiple return instructions). Additional information about the pid provider can be found <a href="http://dtrace.org/blogs/brendan/2011/02/09/dtrace-pid-provider">here</a>.</p>

<h3 id="a-first-dtrace-script-for-ruby">A first DTrace script for Ruby</h3>

<p>Let us now have a look at a first DTrace script for Ruby that will tell us when a Ruby method starts and stops executing, along with the method&rsquo;s execution time. We will be running our DTrace script against the simple Ruby program shown below.</p>

<pre><code class="language-ruby"># sleepy.rb
def even(rnd)
  sleep(rnd)
end

def odd(rnd)
  sleep(rnd)
end

loop do
  rnd = rand(4)
  (rnd % 2 == 0) ? even(rnd) : odd(rnd)
end
</code></pre>

<p>Our simple Ruby program is clearly not going to win any awards. It is just one endless loop, each iteration of which calls a method depending on whether a random number was even or odd. While this is obviously a very contrived example, we can nevertheless make great use of it to illustrate the power of DTrace.</p>

<pre><code class="language-C">/* sleepy.d */
ruby$target:::method-entry
{
  self-&gt;start = timestamp;
  printf(&quot;Entering Method: class: %s, method: %s, file: %s, line: %d\n&quot;, copyinstr(arg0), copyinstr(arg1), copyinstr(arg2), arg3);
}

ruby$target:::method-return
{
  printf(&quot;Returning After: %d nanoseconds\n&quot;, (timestamp - self-&gt;start));
}
</code></pre>

<p>The above DTrace script has us using two Ruby specific DTrace probes. The <code>method-entry</code> probe fires whenever a Ruby method is entered; the <code>method-return</code> probe fires whenever a Ruby method returns. Each probe can take multiple arguments. A probe&rsquo;s arguments are available in the DTrace script through the <code>arg0</code>, <code>arg1</code>, <code>arg2</code> and <code>arg3</code> variables.</p>

<p>If we want to know what data is contained by a probe&rsquo;s arguments, all we have to do is look at <a href="http://ruby-doc.org/core-2.2.3/doc/dtrace_probes_rdoc.html#label-Declared+probes">its documentation</a>. In this particular case, we can see that the <code>method-entry</code> probe gets called by the Ruby process with exactly four arguments.</p>

<blockquote>
<p>ruby:::method-entry(classname, methodname, filename, lineno);</p>

<ul>
<li>classname: name of the class (a string)</li>
<li>methodname: name of the method about to be executed (a string)</li>
<li>filename: the file name where the method is <em>being called</em> (a string)</li>
<li>lineno: the line number where the method is <em>being called</em> (an int)</li>
</ul>
</blockquote>

<p>The documentation tells us that <code>arg0</code> holds the class name, <code>arg1</code> holds the method name, and so on. Equally important is that it tells us the first three arguments are strings, while the fourth one is an integer. We&rsquo;ll need this information for when we want to print any of these arguments with <code>printf</code>.</p>

<p>You probably noticed that we are wrapping string variables inside the <code>copyinstr</code> method. The reason for this is a bit complex. When a string gets passed as an argument to a DTrace probe, we don&rsquo;t actually pass the entire string. Instead, we only pass the memory address where the string begins. This memory address will be specific to the address space of the Ruby process. However, DTrace probes are executed in the kernel and thus make use of a different address space than our Ruby process. In order for a probe to read a string residing in user process data, it first needs to copy this string into the kernel&rsquo;s address space. The <code>copyinstr</code> method is a built-in DTrace function that takes care of this copying for us.</p>

<p>The <code>self-&gt;start</code> notation is interesting as well. DTrace variables starting with <code>self-&gt;</code> are thread-local variables. Thread-local variables are useful when you want to tag every thread that fired a probe with some data. In our case we are using <code>self-&gt;start = timestamp;</code> to tag every thread that triggers the <code>method-entry</code> probe with a thread-local <code>start</code> variable that contains the time in nanoseconds returned by the built-in <code>timestamp</code> method.</p>

<p>While it is impossible for one thread to access the thread-local variables of another thread, it is perfectly possible for a given probe to access the thread-local variables that were set on the current thread by a different probe. Looking at our DTrace script, you can see that the thread-local <code>self-&gt;start</code> variable is being shared between both the <code>method-entry</code> and <code>method-return</code> probes.</p>

<p>Let&rsquo;s go ahead and run the above DTrace script on our Ruby program.</p>

<pre><code class="language-bash">$ sudo dtrace -q -s sleepy.d -c 'ruby sleepy.rb'

Entering Method: class: RbConfig, method: expand, file: /Users/vaneyckt/.rvm/rubies/ruby-2.2.3/lib/ruby/2.2.0/x86_64-darwin14/rbconfig.rb, line: 241
Returning After: 39393 nanoseconds
Entering Method: class: RbConfig, method: expand, file: /Users/vaneyckt/.rvm/rubies/ruby-2.2.3/lib/ruby/2.2.0/x86_64-darwin14/rbconfig.rb, line: 241
Returning After: 12647 nanoseconds
Entering Method: class: RbConfig, method: expand, file: /Users/vaneyckt/.rvm/rubies/ruby-2.2.3/lib/ruby/2.2.0/x86_64-darwin14/rbconfig.rb, line: 241
Returning After: 11584 nanoseconds
...
Entering Method: class: Object, method: odd, file: sleepy.rb, line: 5
Returning After: 1003988894 nanoseconds
Entering Method: class: Object, method: odd, file: sleepy.rb, line: 5
Returning After: 1003887374 nanoseconds
Entering Method: class: Object, method: even, file: sleepy.rb, line: 1
Returning After: 15839 nanoseconds
</code></pre>

<p>It&rsquo;s a bit hard to convey in the snippet above, but our DTrace script is generating well over a thousand lines of output. These lines can be divided into two sections: a first section listing all the Ruby methods being called as part of the program getting ready to run, and a much smaller second section listing whether our program is calling the <code>even</code> or <code>odd</code> functions, along with the time spent in each of these function calls.</p>

<p>While the above output gives us a great amount of detail about what our Ruby program is doing, we really only want to gather information about the <code>even</code> and <code>odd</code> methods being called. DTrace uses predicates to make just this type of filtering possible. Predicates are <code>/</code> wrapped conditions that define whether a particular probe should be executed. The code below shows the usage of predicates to only have the <code>method-entry</code> and <code>method-return</code> probes triggered by the <code>even</code> and <code>odd</code> methods being called.</p>

<pre><code class="language-C">/* predicates_sleepy.d */
ruby$target:::method-entry
/copyinstr(arg1) == &quot;even&quot; || copyinstr(arg1) == &quot;odd&quot;/
{
  self-&gt;start = timestamp;
  printf(&quot;Entering Method: class: %s, method: %s, file: %s, line: %d\n&quot;, copyinstr(arg0), copyinstr(arg1), copyinstr(arg2), arg3);
}

ruby$target:::method-return
/copyinstr(arg1) == &quot;even&quot; || copyinstr(arg1) == &quot;odd&quot;/
{
  printf(&quot;Returning After: %d nanoseconds\n&quot;, (timestamp - self-&gt;start));
}
</code></pre>

<pre><code class="language-bash">$ sudo dtrace -q -s predicates_sleepy.d -c 'ruby sleepy.rb'

Entering Method: class: Object, method: odd, file: sleepy.rb, line: 5
Returning After: 3005086754 nanoseconds
Entering Method: class: Object, method: even, file: sleepy.rb, line: 1
Returning After: 2004313007 nanoseconds
Entering Method: class: Object, method: even, file: sleepy.rb, line: 1
Returning After: 2005076442 nanoseconds
Entering Method: class: Object, method: even, file: sleepy.rb, line: 1
Returning After: 21304 nanoseconds
...
</code></pre>

<p>Running our modified DTrace script, we see that this time around we are only triggering our probes when entering into and returning from the <code>even</code> and <code>odd</code> methods. Now that we have learned a fair few DTrace basics, we can move on to the more advanced topic of writing a DTrace script that will allow us to measure mutex contention in Ruby programs.</p>

<h3 id="monitoring-mutex-contention-with-dtrace">Monitoring mutex contention with DTrace</h3>

<p>The goal of this section is to come up with a DTrace script that measures mutex contention in a multi-threaded Ruby program. This is far from a trivial undertaking and will require us to go and investigate the source code of the Ruby language itself. However, before we get to that, let&rsquo;s first take a look at the Ruby program that we will analyze with the DTrace script that we are going to write in this section.</p>

<pre><code class="language-ruby"># mutex.rb
mutex = Mutex.new
threads = []

threads &lt;&lt; Thread.new do
  loop do
    mutex.synchronize do
      sleep 2
    end
  end
end

threads &lt;&lt; Thread.new do
  loop do
    mutex.synchronize do
      sleep 4
    end
  end
end

threads.each(&amp;:join)
</code></pre>

<p>The above Ruby code starts by creating a mutex object, after which it kicks off two threads. Each thread runs an infinite loop that causes the thread to grab the mutex for a short while before releasing it again. Since the second thread is holding onto the mutex for longer than the first thread, it is intuitively obvious that the first thread will spend a fair amount of time waiting for the second thread to release the mutex.</p>

<p>Our goal is to write a DTrace script that tracks when a given thread has to wait for a mutex to become available, as well as which particular thread is holding the mutex at that point in time. To the best of my knowledge, it is impossible to obtain this contention information by monkey patching the Mutex object, which makes this a great showcase for DTrace. Please get in touch if you think I am wrong on this.</p>

<p>In order for us to write a DTrace script that does the above, we first need to figure out what happens when a thread calls <code>synchronize</code> on a Mutex object. However, mutexes and their methods are implemented as part of the Ruby language itself. This means we are going to have to go and take a look at the <a href="https://github.com/ruby/ruby/tree/ruby_2_2">Ruby MRI source code</a>, which is written in C. Do not worry if you&rsquo;ve never used C. We&rsquo;ll focus on only those parts relevant to our use case.</p>

<p>Let&rsquo;s start at the beginning and look closely at what happens when you call <code>synchronize</code> on a Mutex object. We&rsquo;ll take this step by step:</p>

<ol>
<li><code>synchronize</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L1254">source</a>) calls <code>rb_mutex_synchronize_m</code></li>
<li><code>rb_mutex_synchronize_m</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L502">source</a>) checks if <code>synchronize</code> was called with a block and then goes on to call <code>rb_mutex_synchronize</code></li>
<li><code>rb_mutex_synchronize</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L488">source</a>) calls <code>rb_mutex_lock</code></li>
<li><code>rb_mutex_lock</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L241">source</a>) is where the currently active Ruby thread that executed the <code>mutex.synchronize</code> code will try to grab the mutex</li>
</ol>

<p>There&rsquo;s a lot going on in <code>rb_mutex_lock</code>. The one thing that we are especially interested in is the call to <code>rb_mutex_trylock</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L157">source</a>) on <a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L252">line 252</a>. This method immediately returns <code>true</code> or <code>false</code> depending on whether the Ruby thread managed to grab the mutex. By following the code from line 252 onwards, we can see that <code>rb_mutex_trylock</code> returning <code>true</code> causes <code>rb_mutex_lock</code> to immediately return. On the other hand, <code>rb_mutex_trylock</code> returning <code>false</code> causes <code>rb_mutex_lock</code> to keep executing (and occasionally blocking) until the Ruby thread has managed to get a hold of the mutex.</p>

<p>This is actually all we needed to know in order to be able to go and write our DTrace script. Our investigation showed that when a thread starts executing <code>rb_mutex_lock</code>, this means it wants to acquire a mutex. And when a thread returns from <code>rb_mutex_lock</code>, we know that it managed to successfully obtain this lock. In a previous section, we saw how DTrace allows us to set probes that fire upon entering into or returning from a particular method. We will now use this to write our DTrace script.</p>

<p>Let&rsquo;s go over what exactly our DTrace script should do:</p>

<ol>
<li>when our Ruby program calls <code>mutex.synchronize</code>, we want to make a note of which particular file and line these instructions appear on. We will see later how this allows us to pinpoint problematic code.</li>
<li>when <code>rb_mutex_lock</code> starts executing, we want to write down the current timestamp, as this is when the thread starts trying to acquire the mutex</li>
<li>when <code>rb_mutex_lock</code> returns, we want to compare the current timestamp with the one we wrote down earlier, as this tells us how long the thread had to wait trying to acquire the mutex. We then want to print this duration, along with some information about the location of this particular <code>mutex.synchronize</code> call, to the terminal.</li>
</ol>

<p>Putting it all together, we end up with a DTrace script like shown below.</p>

<pre><code class="language-C">/* mutex.d */
ruby$target:::cmethod-entry
/copyinstr(arg0) == &quot;Mutex&quot; &amp;&amp; copyinstr(arg1) == &quot;synchronize&quot;/
{
  self-&gt;file = copyinstr(arg2);
  self-&gt;line = arg3;
}

pid$target:ruby:rb_mutex_lock:entry
/self-&gt;file != NULL &amp;&amp; self-&gt;line != NULL/
{
  self-&gt;mutex_wait_start = timestamp;
}

pid$target:ruby:rb_mutex_lock:return
/self-&gt;file != NULL &amp;&amp; self-&gt;line != NULL/
{
  mutex_wait_ms = (timestamp - self-&gt;mutex_wait_start) / 1000;
  printf(&quot;Thread %d acquires mutex %d after %d ms - %s:%d\n&quot;, tid, arg1, mutex_wait_ms, self-&gt;file, self-&gt;line);
  self-&gt;file = NULL;
  self-&gt;line = NULL;
}
</code></pre>

<p>The snippet above contains three different probes, the first of which is a Ruby probe that fires whenever a C method is entered. Since the Mutex class and its methods have been implemented in C as <a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L1246-L1255">part of the Ruby MRI</a>, it makes sense for us to use a <code>cmethod-entry</code> probe. Note how we use a predicate to ensure this probe only gets triggered when its first two arguments are &ldquo;Mutex&rdquo; and &ldquo;synchronize&rdquo;. We covered earlier how these arguments <a href="https://ruby-doc.org/core-2.1.0/doc/dtrace_probes_rdoc.html">correspond to the class and method name</a> of the Ruby code that triggered the probe. So this predicate guarantees that this particular probe will only fire when our Ruby code calls the <code>synchronize</code> method on a Mutex object.</p>

<p>The rest of this probe is rather straightforward. The only thing we are doing is storing the file and line number of the Ruby code that triggered the probe into thread-local variables. We are using thread-local variables for two reasons. Firstly, thread-local variables make it trivial to share data with other probes. Secondly, Ruby programs that make use of mutexes will generally be running multiple threads. Using thread-local variables ensures that each Ruby thread will get its own set of probe-specific variables.</p>

<p>Our second probe comes from the pid provider. This provider supplies us with probes for every internal method of a process. In this case we want to use it to get notified whenever <code>rb_mutex_lock</code> starts executing. We saw earlier that a thread will invoke this method when starting to acquire a mutex. The probe itself is pretty simple in that it just stores the current time in a thread-local variable, so as to keep track of when a thread started trying to obtain a mutex. We also use a simple predicate that ensures this probe can only be triggered after the previous probe has fired.</p>

<p>The final probe fires whenever <code>rb_mutex_lock</code> finishes executing. It has a similar predicate as the second probe so as to ensure it can only be triggered after the first probe has fired. We saw earlier how <code>rb_mutex_lock</code> returns whenever a thread has successfully obtained a lock. We can easily calculate the time spent waiting on this lock by comparing the current time with the previously stored <code>self-&gt;mutex_wait_start</code> variable. We then print the time spent waiting, along with the IDs of the current thread and mutex, as well as the location of where the call to <code>mutex.synchronize</code> took place. We finish this probe by assigning <code>NULL</code> to the <code>self-&gt;file</code> and <code>self-&gt;line</code> variables, so as to ensure that the second and third probe can only be triggered after the first one has fired again.</p>

<p>In case you are wondering about how exactly the thread and mutex IDs are obtained, <code>tid</code> is a built-in DTrace variable that identifies the current thread. A <code>pid:::return</code> probe stores the return value of the method that triggered it inside its <code>arg1</code> variable. The <code>rb_mutex_lock</code> method just happens to <a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L306">return an identifier for the mutex that was passed to it</a>, so the <code>arg1</code> variable of this probe does in fact contain the mutex ID.</p>

<p>The final result looks a lot like this.</p>

<pre><code class="language-bash">$ sudo dtrace -q -s mutex.d -c 'ruby mutex.rb'

Thread 286592 acquires mutex 4313316240 after 2 ms - mutex.rb:14
Thread 286591 acquires mutex 4313316240 after 4004183 ms - mutex.rb:6
Thread 286592 acquires mutex 4313316240 after 2004170 ms - mutex.rb:14
Thread 286592 acquires mutex 4313316240 after 6 ms - mutex.rb:14
Thread 286592 acquires mutex 4313316240 after 4 ms - mutex.rb:14
Thread 286592 acquires mutex 4313316240 after 4 ms - mutex.rb:14
Thread 286591 acquires mutex 4313316240 after 16012158 ms - mutex.rb:6
Thread 286592 acquires mutex 4313316240 after 2002593 ms - mutex.rb:14
Thread 286591 acquires mutex 4313316240 after 4001983 ms - mutex.rb:6
Thread 286592 acquires mutex 4313316240 after 2004418 ms - mutex.rb:14
Thread 286591 acquires mutex 4313316240 after 4000407 ms - mutex.rb:6
Thread 286592 acquires mutex 4313316240 after 2004163 ms - mutex.rb:14
Thread 286591 acquires mutex 4313316240 after 4003191 ms - mutex.rb:6
Thread 286591 acquires mutex 4313316240 after 2 ms - mutex.rb:6
Thread 286592 acquires mutex 4313316240 after 4005587 ms - mutex.rb:14
...
</code></pre>

<p>We can get some interesting info about our program just by looking at the above output:</p>

<ol>
<li>there are two threads: 286591 and 286592</li>
<li>both threads try to acquire mutex 4313316240</li>
<li>the mutex acquisition code of the fist thread lives at line 6 of the mutex.rb file</li>
<li>the acquisition code of the second thread is located at line 14 of the same file</li>
<li>there is a lot of mutex contention, with threads having to wait several seconds for the mutex to become available</li>
</ol>

<p>Of course we already knew all of the above was going to happen, as we were familiar with the source code of our Ruby program. The real power of DTrace lies in how we can now go and run our mutex.d script against any Ruby program, no matter how complex, and obtain this level of information without having to read any source code at all. We can even go one step further and run our mutex contention script against an already running Ruby process with <code>sudo dtrace -q -s mutex.d -p &lt;pid&gt;</code>. This can even be run against active production code with minimal overhead.</p>

<p>Before moving on to the next section, I&rsquo;d just like to point out that the above DTrace output actually tells us some cool stuff about how the Ruby MRI schedules threads. If you look at lines 3-6 of the output, you&rsquo;ll notice that the second thread gets scheduled four times in a row. This tells us that when multiple threads are competing for a mutex, the Ruby MRI does not care if a particular thread recently held the mutex.</p>

<h3 id="advanced-mutex-contention-monitoring">Advanced mutex contention monitoring</h3>

<p>We can take the above DTrace script one step further by adding an additional probe that triggers whenever a thread releases a mutex. We will also be slightly altering the output of our script so as to print timestamps rather than durations. While this will make the script&rsquo;s output less suitable for direct human consumption, this timestamp information will make it easier to construct a chronological sequence of the goings-on of our mutexes.</p>

<p>Note that the above doesn&rsquo;t mean that we no longer care about producing output suitable for humans. We&rsquo;ll see how we can easily write a Ruby script to aggregate this new output into something a bit more comprehensive. As an aside, DTrace actually has built-in logic for aggregating data, but I personally prefer to focus my DTrace usage on obtaining data that would otherwise be hard to get, while having my aggregation logic live somewhere else.</p>

<p>Let&rsquo;s start by having a look at how to add a probe that can detect a mutex being released. Luckily, this turns out to be relatively straightforward. It turns out there is a C method called <code>rb_mutex_unlock</code> (<a href="https://github.com/ruby/ruby/blob/325587ee7f76cbcabbc1e6d181cfacb976c39b52/thread_sync.c#L371">source</a>) that releases mutexes. Similarly to <code>rb_mutex_lock</code>, this method returns an identifier to the mutex it acted on. So all we need to do is add a probe that fires whenever <code>rb_mutex_unlock</code> returns. Our final script looks like this.</p>

<pre><code class="language-C">/* mutex.d */
ruby$target:::cmethod-entry
/copyinstr(arg0) == &quot;Mutex&quot; &amp;&amp; copyinstr(arg1) == &quot;synchronize&quot;/
{
  self-&gt;file = copyinstr(arg2);
  self-&gt;line = arg3;
}

pid$target:ruby:rb_mutex_lock:entry
/self-&gt;file != NULL &amp;&amp; self-&gt;line != NULL/
{
  printf(&quot;Thread %d wants to acquire mutex %d at %d - %s:%d\n&quot;, tid, arg1, timestamp, self-&gt;file, self-&gt;line);
}

pid$target:ruby:rb_mutex_lock:return
/self-&gt;file != NULL &amp;&amp; self-&gt;line != NULL/
{
  printf(&quot;Thread %d has acquired mutex %d at %d - %s:%d\n&quot;, tid, arg1, timestamp, self-&gt;file, self-&gt;line);
  self-&gt;file = NULL;
  self-&gt;line = NULL;
}

pid$target:ruby:rb_mutex_unlock:return
{
  printf(&quot;Thread %d has released mutex %d at %d\n&quot;, tid, arg1, timestamp);
}
</code></pre>

<pre><code class="language-bash">$ sudo dtrace -q -s mutex.d -c 'ruby mutex.rb'

Thread 500152 wants to acquire mutex 4330240800 at 53341356615492 - mutex.rb:6
Thread 500152 has acquired mutex 4330240800 at 53341356625449 - mutex.rb:6
Thread 500153 wants to acquire mutex 4330240800 at 53341356937292 - mutex.rb:14
Thread 500152 has released mutex 4330240800 at 53343360214311
Thread 500152 wants to acquire mutex 4330240800 at 53343360266121 - mutex.rb:6
Thread 500153 has acquired mutex 4330240800 at 53343360301928 - mutex.rb:14
Thread 500153 has released mutex 4330240800 at 53347365475537
Thread 500153 wants to acquire mutex 4330240800 at 53347365545277 - mutex.rb:14
Thread 500152 has acquired mutex 4330240800 at 53347365661847 - mutex.rb:6
Thread 500152 has released mutex 4330240800 at 53349370397555
Thread 500152 wants to acquire mutex 4330240800 at 53349370426972 - mutex.rb:6
Thread 500153 has acquired mutex 4330240800 at 53349370453489 - mutex.rb:14
Thread 500153 has released mutex 4330240800 at 53353374785751
Thread 500153 wants to acquire mutex 4330240800 at 53353374834184 - mutex.rb:14
Thread 500152 has acquired mutex 4330240800 at 53353374868435 - mutex.rb:6
...
</code></pre>

<p>The above output is pretty hard to parse for a human reader. The snippet below shows a Ruby program that aggregates this data into a more readable format. I&rsquo;m not going to go into the details of this Ruby program as it is essentially just a fair bit of string filtering with some bookkeeping to help keep track of how each thread interacts with the mutexes and the contention this causes.</p>

<pre><code class="language-ruby"># aggregate.rb
mutex_owners     = Hash.new
mutex_queuers    = Hash.new { |h,k| h[k] = Array.new }
mutex_contention = Hash.new { |h,k| h[k] = Hash.new(0) }

time_of_last_update = Time.now
update_interval_sec = 1

ARGF.each do |line|
  # when a thread wants to acquire a mutex
  if matches = line.match(/^Thread (\d+) wants to acquire mutex (\d+) at (\d+) - (.+)$/)
    captures  = matches.captures
    thread_id = captures[0]
    mutex_id  = captures[1]
    timestamp = captures[2].to_i
    location  = captures[3]

    mutex_queuers[mutex_id] &lt;&lt; {
      thread_id: thread_id,
      location:  location,
      timestamp: timestamp
    }
  end

  # when a thread has acquired a mutex
  if matches = line.match(/^Thread (\d+) has acquired mutex (\d+) at (\d+) - (.+)$/)
    captures  = matches.captures
    thread_id = captures[0]
    mutex_id  = captures[1]
    timestamp = captures[2].to_i
    location  = captures[3]

    # set new owner
    mutex_owners[mutex_id] = {
      thread_id: thread_id,
      location: location
    }

    # remove new owner from list of queuers
    mutex_queuers[mutex_id].delete_if do |queuer|
      queuer[:thread_id] == thread_id &amp;&amp;
      queuer[:location] == location
    end
  end

  # when a thread has released a mutex
  if matches = line.match(/^Thread (\d+) has released mutex (\d+) at (\d+)$/)
    captures  = matches.captures
    thread_id = captures[0]
    mutex_id  = captures[1]
    timestamp = captures[2].to_i

    owner_location = mutex_owners[mutex_id][:location]

    # calculate how long the owner caused each queuer to wait
    # and change queuer timestamp to the current timestamp in preparation
    # for the next round of queueing
    mutex_queuers[mutex_id].each do |queuer|
      mutex_contention[owner_location][queuer[:location]] += (timestamp - queuer[:timestamp])
      queuer[:timestamp] = timestamp
    end
  end

  # print mutex contention information
  if Time.now - time_of_last_update &gt; update_interval_sec
    system('clear')
    time_of_last_update = Time.now

    puts 'Mutex Contention'
    puts &quot;================\n\n&quot;

    mutex_contention.each do |owner_location, contention|
      puts owner_location
      owner_location.length.times { print '-' }
      puts &quot;\n&quot;

      total_duration_sec = 0.0
      contention.sort.each do |queuer_location, queueing_duration|
        duration_sec = queueing_duration / 1000000000.0
        total_duration_sec += duration_sec
        puts &quot;#{queuer_location}\t#{duration_sec}s&quot;
      end
      puts &quot;total\t\t#{total_duration_sec}s\n\n&quot;
    end
  end
end
</code></pre>

<pre><code class="language-bash">$ sudo dtrace -q -s mutex.d -c 'ruby mutex.rb' | ruby aggregate.rb


Mutex Contention
================

mutex.rb:6
----------
mutex.rb:14	  10.016301065s
total		  10.016301065s

mutex.rb:14
-----------
mutex.rb:6	  16.019252339s
total		  16.019252339s
</code></pre>

<p>The final result looks like shown above. Note that our program will clear the terminal every second before printing summarized contention information. Here we see that after running the program for a bit, <code>mutex.rb:6</code> caused <code>mutex.rb:14</code> to spend about 10 seconds waiting for the mutex to become available. The <code>total</code> field indicates the total amount of waiting across all other threads caused by <code>mutex.rb:6</code>. This number becomes more useful when there are more than two threads competing for a single mutex.</p>

<p>I want to stress that while the example shown here was kept simple on purpose, our code is in fact more than capable of handling more complex scenarios. For example, let&rsquo;s have a look at some Ruby code that uses multiple mutexes, some of which are nested.</p>

<pre><code class="language-ruby"># mutex.rb
mutexes =[Mutex.new, Mutex.new]
threads = []

threads &lt;&lt; Thread.new do
  loop do
    mutexes[0].synchronize do
      sleep 2
    end
  end
end

threads &lt;&lt; Thread.new do
  loop do
    mutexes[1].synchronize do
      sleep 2
    end
  end
end

threads &lt;&lt; Thread.new do
  loop do
    mutexes[1].synchronize do
      sleep 1
    end
  end
end

threads &lt;&lt; Thread.new do
  loop do
    mutexes[0].synchronize do
      sleep 1
      mutexes[1].synchronize do
        sleep 1
      end
    end
  end
end

threads.each(&amp;:join)
</code></pre>

<pre><code class="language-bash">$ sudo dtrace -q -s mutex.d -c 'ruby mutex.rb' | ruby aggregate.rb


Mutex Contention
================

mutex.rb:6
----------
mutex.rb:30	  36.0513079s
total		  36.0513079s

mutex.rb:14
-----------
mutex.rb:22	  78.123187353s
mutex.rb:32	  36.062005125s
total		  114.185192478s

mutex.rb:22
-----------
mutex.rb:14	  38.127435904s
mutex.rb:32	  19.060814411s
total		  57.188250315000005s

mutex.rb:32
-----------
mutex.rb:14	  24.073966949s
mutex.rb:22	  24.073383955s
total		  48.147350904s

mutex.rb:30
-----------
mutex.rb:6	  103.274153073s
total		  103.274153073s
</code></pre>

<p>The above output tells us very clearly that we should concentrate our efforts on lines 14 and 30 when we want to try to make our program faster. The really cool thing about all this is that this approach will work regardless of the complexity of your program and requires absolutely no familiarity with the source code at all. You can literally run this against code you&rsquo;ve never seen and walk away with a decent idea of where the mutex bottlenecks are located. And on top of that, since we are using DTrace, we don&rsquo;t even have to add any instrumentation code to the program we want to investigate. Instead, we can just run this against an already active process without even having to interrupt it.</p>

<h3 id="conclusion">Conclusion</h3>

<p>I hope to have convinced you that DTrace is a pretty amazing tool that can open up whole new ways of trying to approach a problem. There is so so much I haven&rsquo;t even touched on yet. The topic is just too big to cover in a single post. If you&rsquo;re interested in learning DTrace, here are some resources I can recommend:</p>

<ul>
<li>the <a href="https://github.com/opendtrace/toolkit">DTrace Toolkit</a> is a curated collection of DTrace scripts for various systems</li>
<li>I often find myself peeking at the <a href="http://www.tablespace.net/quicksheet/dtrace-quickstart.html">DTrace QuickStart</a> and the <a href="http://www.brendangregg.com/DTrace/DTrace-cheatsheet.pdf">DTrace Cheatsheet</a> when I can&rsquo;t quite remember how something works</li>
<li>the first chapters of <a href="https://www.amazon.com/DTrace-Dynamic-Tracing-Solaris-FreeBSD/dp/0132091518">DTrace: Dynamic Tracing in Oracle Solaris, Mac OS X and FreeBSD</a> act as a DTrace tutorial. The rest of the book is all about how to use DTrace to solve real-life scenarios with tons and tons of examples.</li>
<li>I really wish I had come across <a href="https://awesome-dtrace.com/">Awesome DTrace</a> earlier. It&rsquo;s a curated list about various DTrace topics, some of which I had not been able to find any information about before.</li>
</ul>

<p>Just one more thing before I finish this. If you&rsquo;re on OS X and encounter DTrace complaining about not being able to control executables signed with restricted entitlements, be aware that you can easily work around this by using the <code>-p</code> parameter to directly specify the pid of the process you want DTrace to run against. Please contact me if you manage to find the proper fix for this.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/ruby_concurrency_in_praise_of_the_mutex/">
                Ruby concurrency: in praise of the mutex
              </a>
            </h1>

            <span class="post-date">Mar 17, 2016</span>

            

            

<p>When reading about Ruby you will inevitably be introduced to the Global Interpreter Lock. This mechanism tends to come up in explanations of why Ruby threads run concurrently on a single core, rather than being scheduled across multiple cores in true parallel fashion. This single core scheduling approach also explains why adding threads to a Ruby program does not necessarily result in faster execution times.</p>

<p>This post will start by explaining some of the details behind the GIL. Next up, we&rsquo;ll take a look at the three crucial concepts of concurrency: atomicity, visibility, and ordering. While most developers are familiar with atomicity, the concept of visibility is often not very well understood. We will be going over these concepts in quite some detail and will illustrate how to address their needs through correct usage of the mutex data structure.</p>

<h3 id="parallelism-and-the-gil">Parallelism and the GIL</h3>

<p>Ruby&rsquo;s Global Interpreter Lock is a global lock around the execution of Ruby code. Before a Ruby thread can execute any code, it first needs to acquire this lock. A thread holding the GIL will be forced to release it after a certain amount of time, at which point the kernel can hand the GIL to another Ruby thread. As the GIL can only be held by one thread at a time, it effectively prevents two Ruby threads from being executed at the same time.</p>

<p>Luckily Ruby comes with an optimization that forces threads to let go off the GIL when they find themselves waiting on blocking IO to complete. Such threads will use the <a href="http://linux.die.net/man/2/ppoll">ppoll system call</a> to be notified when their blocking IO has finished. Only then will they make an attempt to reacquire the GIL again. This type of behavior holds true for all blocking IO calls, as well as backtick and system calls. So even with the Global Interpreter Lock, Ruby is still able to have moments of true parallelism.</p>

<p>Note that the GIL is specific to the default Ruby interpreter (<a href="https://en.wikipedia.org/wiki/Ruby_MRI">MRI</a>) which relies on a global lock to protect its internals from race conditions. The GIL also makes it possible to safely interface the MRI interpreter with C libraries that may not be thread-safe themselves. Other interpreters have taken different approaches to the concept of a global lock; <a href="http://rubinius.com/">Rubinius</a> opts for a collection of fine-grained locks instead of a single global one, whereas <a href="http://jruby.org/">JRuby</a> does not use global locking at all.</p>

<h3 id="concurrency-and-the-mutex">Concurrency and the Mutex</h3>

<p>There are three crucial concepts to concurrency: atomicity, visibility, and ordering. We&rsquo;ll be taking a look at how Ruby&rsquo;s mutex data structure addresses these. It is worth pointing out that different languages tackle these concepts in different ways. As such, the mutex-centric approach described here is only guaranteed to work in Ruby.</p>

<h4 id="atomicity">Atomicity</h4>

<p>Atomicity is probably the best-known concurrency concept. A section of code is said to atomically modify the state of an object if all other threads are unable to see any of the intermediate states of the object being modified. These other threads either see the state as it was before the operation, or they see the state as it is after the operation.</p>

<p>In the example below we have created a <code>counters</code> array that holds ten entries, each of which is set to zero. This array represents an object that we want to modify, and its entries represent its internal state. Let&rsquo;s say we have five threads, each of which executes a loop for 100.000 iterations that increments every entry by one. Intuitively we&rsquo;d expect the output of this to be an array with each entry set to 500.000. However, as we can see below, this is not the case.</p>

<pre><code class="language-ruby"># atomicity.rb
counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

threads = 5.times.map do
  Thread.new do
    100000.times do
      counters.map! { |counter| counter + 1 }
    end
  end
end
threads.each(&amp;:join)

puts counters.to_s
# =&gt; [500000, 447205, 500000, 500000, 500000, 500000, 203656, 500000, 500000, 500000]
</code></pre>

<p>The reason for this unexpected output is that <code>counters.map! { |counter| counter + 1 }</code> is not atomic. For example, imagine that our first thread has just read the value of the first entry, incremented it by one, and is now getting ready to write this incremented value to the first entry of our array. However, before our thread can write this incremented value, it gets interrupted by the second thread. This second thread then goes on to read the current value of the first entry, increments it by one, and succeeds in writing the result back to the first entry of our array. Now we have a problem!</p>

<p>We have a problem because the first thread got interrupted before it had a chance to write its incremented value to the array. When the first thread resumes, it will end up overwriting the value that the second thread just placed in the array. This will cause us to essentially lose an increment operation, which explains why our program output has entries in it that are less than 500.000.</p>

<p>It should hopefully be clear that none of this would have happened if we had made sure that <code>counters.map! { |counter| counter + 1 }</code> was atomic. This would have made it impossible for the second thread to just come in and modify the intermediate state of the <code>counters</code> array.</p>

<pre><code class="language-ruby"># atomicity.rb
mutex = Mutex.new
counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

threads = 5.times.map do
  Thread.new do
    100000.times do
      mutex.synchronize do
        counters.map! { |counter| counter + 1 }
      end
    end
  end
end
threads.each(&amp;:join)

puts counters.to_s
# =&gt; [500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000]
</code></pre>

<p>Atomicity can be accomplished by using a mutex as a locking mechanism that ensures no two threads can simultaneously execute the same section of code. The code above shows how we can prevent a thread executing <code>counters.map! { |counter| counter + 1 }</code> from being interrupted by other threads wanting to execute the same code. Also, be sure to note that <code>mutex.synchronize</code> only prevents a thread from being interrupted by others wanting to execute code wrapped inside the same <code>mutex</code> variable!</p>

<h4 id="visibility">Visibility</h4>

<p>Visibility determines when the results of the actions performed by a thread become visible to other threads. For example, when a thread wants to write an updated value to memory, that updated value may end up being put in a cache for a while until the kernel decides to flush it to main memory. Other threads that read from that memory will therefore end up with a stale value!</p>

<p>The code below shows an example of the visibility problem. Here we have several threads flipping the boolean values in the <code>flags</code> array over and over again. The code responsible for changing these values is wrapped inside a mutex, so we know the intermediate states of the <code>flags</code> array won&rsquo;t be visible to other threads. We would thus expect the output of this program to contain the same boolean value for every entry of this array. However, we shall soon see that this does not always hold true.</p>

<pre><code class="language-ruby"># visibility.rb
mutex = Mutex.new
flags = [false, false, false, false, false, false, false, false, false, false]

threads = 50.times.map do
  Thread.new do
    100000.times do
      puts flags.to_s
      mutex.synchronize do
        flags.map! { |f| !f }
      end
    end
  end
end
threads.each(&amp;:join)
</code></pre>

<pre><code class="language-bash">$ ruby visibility.rb &gt; visibility.log
$ grep -Hnri 'true, false' visibility.log | wc -l
    30
</code></pre>

<p>This code will produce five million lines of output. We&rsquo;ll use the <code>&gt;</code> operator to write all these lines to a file. Having done this, we can then <code>grep</code> for inconsistencies in the output. We would expect every line of the output to contain an array with all its entries set to the same boolean value. However, it turns out that this only holds true for 99.9994% of all lines. Sometimes the flipped boolean values don&rsquo;t get written to memory fast enough, causing other threads to read stale data. This is a great illustration of the visibility problem.</p>

<p>Luckily we can solve this problem by using a <a href="https://en.wikipedia.org/wiki/Memory_barrier">memory barrier</a>. A memory barrier enforces an ordering constraint on memory operations thereby preventing the possibility of reading stale data. In Ruby, a mutex not only acts as an atomic lock, but also functions as a memory barrier. When wanting to read the value of a variable being modified by multiple threads, a memory barrier will effectively tell your program to wait until all in-flight memory writes are complete. In practice this means that if we use a mutex when writing to a variable, we need to use this same mutex when reading from that variable as well.</p>

<pre><code class="language-ruby"># visibility.rb
mutex = Mutex.new
flags = [false, false, false, false, false, false, false, false, false, false]

threads = 50.times.map do
  Thread.new do
    100000.times do
      mutex.synchronize do
        puts flags.to_s
      end
      mutex.synchronize do
        flags.map! { |f| !f }
      end
    end
  end
end
threads.each(&amp;:join)
</code></pre>

<pre><code class="language-bash">$ ruby visibility.rb &gt; visibility.log
$ grep -Hnri 'true, false' visibility.log | wc -l
    0
</code></pre>

<p>As expected, this time we found zero inconsistencies in the output data due to us using the same mutex for both reading and writing the boolean values of the <code>flags</code> array. Do keep in mind that not all languages allow for using a mutex as a memory barrier, so be sure to check the specifics of your favorite language before going off to write concurrent code.</p>

<h4 id="ordering">Ordering</h4>

<p>As if dealing with visibility isn&rsquo;t hard enough, the Ruby interpreter is also allowed to change the order of the instructions in your code in an attempt at optimization. Before I continue I should point out that there is no official specification for the Ruby language. This can make it hard to find information about topics such as this. So I&rsquo;m just going to describe how I <em>think</em> instruction reordering currently works in Ruby.</p>

<p>Your Ruby code gets compiled to bytecode by the Ruby interpreter. The interpreter is free to reorder your code in an attempt to optimize it. This bytecode will then generate a set of CPU instructions, which <a href="https://en.wikipedia.org/wiki/Out-of-order_execution">the CPU is free to reorder</a> as well. I wasn&rsquo;t able to come up with example code that actually showcases this reordering behavior, so this next bit is going to be somewhat hand-wavy. Let&rsquo;s say we were given the code shown below (<a href="http://jeremymanson.blogspot.ie/2007/08/atomicity-visibility-and-ordering.html">original source</a>).</p>

<pre><code class="language-ruby"># ordering.rb
a = false
b = false
threads = []

thr1 = Thread.new do
  a = true
  b = true
end

thr2 = Thread.new do
  r1 = b # could see true
  r2 = a # could see false
  r3 = a # could see true
  puts (r1 &amp;&amp; !r2) &amp;&amp; r3 # could print true
end

thr1.join
thr2.join
</code></pre>

<p>Since there are a lot of ways for instruction reordering to take place, it is not impossible for <code>b = true</code> to be executed before <code>a = true</code>. In theory, this could therefore allow for <code>thr2</code> to end up outputting <code>true</code>. This is rather counterintuitive, as this would only be possible if the variable <code>b</code> had changed value before the variable <code>a</code>.</p>

<p>Luckily there is no need to worry too much about this. When looking at the code above, it should be obvious that code reordering is going to be the least of its problems. The lack of any kind of synchronization to help deal with atomicity and visibility issues in this threaded program is going to cause way bigger headaches than code reordering ever could.</p>

<p>Those synchronization issues can be fixed by using a mutex. By introducing a mutex we are explicitly telling the interpreter and CPU how our code should behave, thus preventing any problematic code reordering from occurring. Dealing with atomicity and visibility issues will therefore implicitly prevent any dangerous code reordering.</p>

<h3 id="conclusion">Conclusion</h3>

<p>I hope this post has helped show just how easy it can be to introduce bugs in concurrent code. In my experience, the concept of memory barriers is often poorly understood, which can result in introducing some incredibly hard to find bugs. Luckily, as we saw in this post, the mutex data structure can be a veritable panacea for addressing these issues in Ruby.</p>

<p>Please feel free to contact me if you think I got anything wrong. While all of the above is correct to the best of my knowledge, the lack of an official Ruby specification can make it hard to locate information that is definitively without error.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/how_to_write_your_own_rspec_retry_mechanism/">
                How to write your own rspec retry mechanism
              </a>
            </h1>

            <span class="post-date">Jan 17, 2016</span>

            

            

<p>Imagine you have an rspec test suite filled with <a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html">system tests</a>. Each system test simulates how a real user would interact with your app by opening a browser session through which it fills out text fields, clicks on buttons, and sends data to public endpoints. Unfortunately, browser drivers are not without bugs and sometimes your tests will fail because of these. Wouldn&rsquo;t it be nice if we could automatically retry these failed tests?</p>

<p>This article starts by investigating how rspec formatters can be used to help us keep track of failed tests. Next, we&rsquo;ll use this information to take a first stab at creating a rake task that can automatically retry failed tests. Lastly, we&rsquo;ll explore how to further improve our simple rake task so as to make it ready for use in production.</p>

<p>Note that any code shown in this post is only guaranteed to work with rspec 3.3. In the past I&rsquo;ve written similar code for other rspec versions as well though. So don&rsquo;t worry, it shouldn&rsquo;t be too hard to get all of this to work on whatever rspec version you find yourself using.</p>

<h3 id="rspec-formatters">Rspec formatters</h3>

<p>Rspec generates its command line output by relying on formatters that receive messages on events like <code>example_passed</code> and <code>example_failed</code>. We can use these hooks to help us keep track of failed tests by having them write the descriptions of failed tests to a text file named <code>tests_failed</code>. Our <code>FailureFormatter</code> class does just that.</p>

<pre><code class="language-ruby"># failure_formatter.rb
require 'rspec/core/formatters/progress_formatter'

class FailureFormatter &lt; RSpec::Core::Formatters::ProgressFormatter
  RSpec::Core::Formatters.register self, :example_failed

  def example_failed(notification)
    super
    File.open('tests_failed', 'a') do |file|
      file.puts(notification.example.full_description)
    end
  end
end
</code></pre>

<p>We&rsquo;ll soon have a look at how tests behave when we try to run them with the formatter shown above. But first, let&rsquo;s prepare some example tests. We&rsquo;ll create two tests. One of which will always pass, and another one which will always fail.</p>

<pre><code class="language-ruby"># my_fake_tests_spec.rb
describe 'my fake tests', :type =&gt; :feature do

  it 'this scenario should pass' do
    expect(true).to eq true
  end

  it 'this scenario should fail' do
    expect(false).to eq true
  end
end
</code></pre>

<p>Having done that, we can now run our tests with the <code>FailureFormatter</code> we wrote earlier. As you can see below, we&rsquo;ll have to pass both <code>--require</code> and <code>--format</code> params in order to get our formatter to work. I&rsquo;m also using the <code>--no-fail-fast</code> flag so as to prevent our test suite from exiting upon encountering its first failure.</p>

<pre><code class="language-bash">$ bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
.F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &lt;top (required)&gt;'

Finished in 0.02272 seconds (files took 0.0965 seconds to load)
2 examples, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail
</code></pre>

<p>After running this, we should now have a <code>tests_failed</code> file that contains a single line describing our failed test. As we can see in the snippet below, this is indeed the case.</p>

<pre><code class="language-bash">$ cat tests_failed

my fake tests this scenario should fail
</code></pre>

<p>Take a moment to reflect on what we have just done. By writing just a few lines of code we have effectively created a logging mechanism that will help us keep track of failed tests. In the next section we will look at how we can make use of this mechanism to automatically rerun failed tests.</p>

<h3 id="first-pass-at-creating-the-retry-task">First pass at creating the retry task</h3>

<p>In this section we will create a rake task that runs our rspec test suite and automatically retries any failed tests. The finished rake task is shown below. For now, have a look at this code and then we&rsquo;ll go over its details in the next few paragraphs.</p>

<pre><code class="language-ruby">require 'fileutils'

task :rspec_with_retries, [:max_tries] do |_, args|
  max_tries = args[:max_tries].to_i

  # construct initial rspec command
  command = 'bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast'

  max_tries.times do |t|
    puts &quot;\n&quot;
    puts '##########'
    puts &quot;### STARTING TEST RUN #{t + 1} OUT OF A MAXIMUM OF #{max_tries}&quot;
    puts &quot;### executing command: #{command}&quot;
    puts '##########'

    # delete tests_failed file left over by previous run
    FileUtils.rm('tests_failed', :force =&gt; true)

    # run tests
    puts `#{command}`

    # early out
    exit 0 if $?.exitstatus.zero?
    exit 1 if (t == max_tries - 1)

    # determine which tests need to be run again
    failed_tests = []
    File.open('tests_failed', 'r') do |file|
      failed_tests = file.readlines.map { |line| &quot;\&quot;#{line.strip}\&quot;&quot; }
    end

    # construct command to rerun just the failed tests
    command  = ['bundle exec rspec']
    command += Array.new(failed_tests.length, '-e').zip(failed_tests).flatten
    command += ['--require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast']
    command = command.join(' ')
  end
end
</code></pre>

<p>The task executes the <code>bundle exec rspec</code> command a <code>max_tries</code> number of times. The first iteration runs the full rspec test suite with the <code>FailureFormatter</code> class and writes the descriptions of failed tests to a <code>tests_failed</code> file. Subsequent iterations read from this file and use the <a href="https://relishapp.com/rspec/rspec-core/v/3-3/docs/command-line/example-option">-e option</a> to rerun the tests listed there.</p>

<p>Note that these subsequent iterations use the <code>FailureFormatter</code> as well. This means that any tests that failed during the second iteration will get written to the <code>tests_failed</code> file to be retried by the third iteration. This continues until we reach the max number of iterations or until one of our iterations has all its tests pass.</p>

<p>Every iteration deletes the <code>tests_failed</code> file from the previous iteration. For this we use the <code>FileUtils.rm</code> method with the <code>:force</code> flag set to <code>true</code>. This flag ensures that the program doesn&rsquo;t crash in case the <code>tests_failed</code> file doesn&rsquo;t exist. The above code relies on backticks to execute the <code>bundle exec rspec</code> subprocess. Because of this we need to use the global variable <code>$?</code> to access the exit status of this subprocess.</p>

<p>Below you can see the output of a run of our rake task. Notice how the first iteration runs both of our tests, whereas the second and third iterations rerun just the failed test. This shows our retry mechanism is indeed working as expected.</p>

<pre><code class="language-bash">$ rake rspec_with_retries[3]

##########
### STARTING TEST RUN 1 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
.F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &lt;top (required)&gt;'

Finished in 0.02272 seconds (files took 0.0965 seconds to load)
2 examples, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail


##########
### STARTING TEST RUN 2 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec -e &quot;my fake tests this scenario should fail&quot; --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
Run options: include {:full_description=&gt;/my\ fake\ tests\ this\ scenario\ should\ fail/}
F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &lt;top (required)&gt;'

Finished in 0.02286 seconds (files took 0.09094 seconds to load)
1 example, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail


##########
### STARTING TEST RUN 3 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec -e &quot;my fake tests this scenario should fail&quot; --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
Run options: include {:full_description=&gt;/my\ fake\ tests\ this\ scenario\ should\ fail/}
F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &lt;top (required)&gt;'

Finished in 0.02378 seconds (files took 0.09512 seconds to load)
1 example, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail
</code></pre>

<p>The goal of this section was to introduce the general idea behind our retry mechanism. There are however several shortcomings in the code that we&rsquo;ve shown here. The next section will focus on identifying and fixing these.</p>

<h3 id="perfecting-the-retry-task">Perfecting the retry task</h3>

<p>The code in the previous section isn&rsquo;t all that bad, but there are a few things related to the <code>bundle exec rspec</code> subprocess that we can improve upon. In particular, using backticks to initiate subprocesses has several downsides:</p>

<ul>
<li>the standard output stream of the subprocess gets written into a buffer which we cannot print until the subprocess finishes</li>
<li>the standard error stream does not even get written to this buffer</li>
<li>the backticks approach does not return the id of the subprocess to us</li>
</ul>

<p>This last downside is especially bad as not having the subprocess id makes it hard for us to cancel the subprocess in case the rake task gets terminated. This is why I prefer to use the <a href="https://github.com/jarib/childprocess">childprocess gem</a> for handling subprocesses instead.</p>

<pre><code class="language-ruby">require 'fileutils'
require 'childprocess'

task :rspec_with_retries, [:max_tries] do |_, args|
  max_tries = args[:max_tries].to_i

  # exit hook to ensure rspec process gets stopped when CTRL+C (SIGTERM is pressed)
  # needs to be set outside the times loop as otherwise each iteration would add its
  # own at_exit hook
  process = nil
  at_exit do
    process.stop unless process.nil?
  end

  # construct initial rspec command
  command = ['bundle', 'exec', 'rspec', '--require', './spec/formatters/failure_formatter.rb', '--format', 'FailureFormatter', '--no-fail-fast']

  max_tries.times do |t|
    puts &quot;\n&quot;
    puts '##########'
    puts &quot;### STARTING TEST RUN #{t + 1} OUT OF A MAXIMUM OF #{max_tries}&quot;
    puts &quot;### executing command: #{command}&quot;
    puts '##########'

    # delete tests_failed file left over by previous run
    FileUtils.rm('tests_failed', :force =&gt; true)

    # run tests in separate process
    process = ChildProcess.build(*command)
    process.io.inherit!
    process.start
    process.wait

    # early out
    exit 0 if process.exit_code.zero?
    exit 1 if (t == max_tries - 1)

    # determine which tests need to be run again
    failed_tests = []
    File.open('tests_failed', 'r') do |file|
      failed_tests = file.readlines.map { |line| line.strip }
    end

    # construct command to rerun just the failed tests
    command  = ['bundle', 'exec', 'rspec']
    command += Array.new(failed_tests.length, '-e').zip(failed_tests).flatten
    command += ['--require', './spec/formatters/failure_formatter.rb', '--format', 'FailureFormatter', '--no-fail-fast']
  end
end
</code></pre>

<p>As we can see from the line <code>process = ChildProcess.build(*command)</code>, this gem makes it trivial to obtain the subprocess id. This then allows us to write an <code>at_exit</code> hook that shuts this subprocess down upon termination of our rake task. For example, using ctrl+c to cease the rake task will now cause the rspec subprocess to stop as well.</p>

<p>This gem also makes it super easy to inherit the stdout and stderr streams from the parent process (our rake task). This means that anything that gets written to the stdout and stderr streams of the subprocess will now be written directly to the stdout and stderr streams of our rake task. Or in other words, our rspec subprocess is now able to output directly to the rake task&rsquo;s terminal session. Having made these improvements, our <code>rspec_with_retries</code> task is now ready for use in production.</p>

<h3 id="conclusion">Conclusion</h3>

<p>I hope this post helped some people out there who find themselves struggling to deal with flaky tests. Please note that a retry mechanism such as this is really only possible because of rspec&rsquo;s powerful formatters. Get in touch if you have any examples of other cool things built on top of this somewhat underappreciated feature!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/the_disaster_that_is_rubys_timeout_method/">
                The disaster that is Ruby&#39;s timeout method
              </a>
            </h1>

            <span class="post-date">Dec 19, 2015</span>

            

            

<p>On paper, <a href="http://ruby-doc.org/stdlib-2.1.1/libdoc/timeout/rdoc/Timeout.html#method-c-timeout">Ruby&rsquo;s timeout method</a> looks like an incredibly useful piece of code. Ever had a network request occasionally slow down your entire program because it just wouldn&rsquo;t finish? That&rsquo;s where <code>timeout</code> comes in. It provides a hard guarantee that a block of code will be finished within a specified amount of time.</p>

<pre><code class="language-ruby">require 'timeout'

timeout(5) do
  # block of code that should be interrupted if it takes more than 5 seconds
end
</code></pre>

<p>There&rsquo;s one thing the documentation doesn&rsquo;t tell you though. If any of the lines in that block of code introduces side effects that rely on the execution of later lines of code to leave things in a stable state, then using the <code>timeout</code> method is a great way to introduce instability in your program. Examples of this include pretty much any program that is not entirely without stateful information. Let&rsquo;s have a closer look at this method to try and figure out what&rsquo;s going on here exactly.</p>

<h3 id="exceptions-absolutely-anywhere">Exceptions absolutely anywhere</h3>

<p>The problem with <code>timeout</code> is that it relies upon Ruby&rsquo;s questionable ability to have one thread raise an exception <em>absolutely anywhere</em> in an entirely different thread. The idea is that when you place code inside a <code>timeout</code> block, this code gets wrapped inside a new thread that executes in the background while the main thread goes to sleep for 5 seconds. Upon waking, the main thread grabs the background thread and forcefully stops it by raising a <code>Timeout::Error</code> exception on it (<a href="https://github.com/ruby/ruby/blob/trunk/lib/timeout.rb#L72-L110">actual implementation</a>).</p>

<pre><code class="language-ruby"># raising_exceptions.rb
# threads can raise exceptions in other threads
thr = Thread.new do
  puts '...initializing resource'
  sleep 1

  puts '...using resource'
  sleep 1

  puts '...cleaning resource'
  sleep 1
end

sleep 1.5
thr.raise('raising an exception in the thread')
</code></pre>

<pre><code class="language-bash">$ ruby raising_exeptions.rb

...initializing resource
...using resource
</code></pre>

<p>The problem with this approach is that the main thread does not care what code the background thread is executing when it raises the exception. This means that the engineer responsible for the code that gets executed by the background thread needs to assume an exception can get thrown from <em>absolutely anywhere</em> within her code. This is madness! No one can be expected to place exception catchers around every single block of code!</p>

<p>The following code further illustrates the problem of being able to raise an exception <em>absolutely anywhere</em>. Turns out that <em>absolutely anywhere</em> includes locations like the inside of <code>ensure</code> blocks. These locations are generally not designed for handling any exceptions at all. I hope you weren&rsquo;t using an <code>ensure</code> block to terminate your database connection!</p>

<pre><code class="language-ruby"># ensure_block.rb
# raising exceptions inside an ensure block of another thread
# note how we never finish cleaning the resource here
thr = Thread.new do
  begin
    puts '...initializing resource'
    sleep 1

    raise 'something went wrong'

    puts '...using resource'
    sleep 1
  ensure
    puts '...started cleaning resource'
    sleep 1
    puts '...finished cleaning resource'
  end
end

sleep 1.5
thr.raise('raising an exception in the thread')

# prevent program from immediately terminating after raising exception
sleep 5
</code></pre>

<pre><code class="language-bash">$ ruby ensure_blocks.rb

...initializing resource
...started cleaning resource
</code></pre>

<h3 id="real-world-example">Real world example</h3>

<p>Recently, I spent a lot of time working with the <a href="https://github.com/taf2/curb">curb http client</a>. I ended up wrapping quite a few of my curb calls within <code>timeout</code> blocks because of tight time constraints. However, this caused great instability within the system I was working on. Sometimes a call would work, whereas other times that very same call would throw an exception about an invalid handle. It was this that caused me to start investigating the <code>timeout</code> method.</p>

<p>After having a bit of think, I came up with a proof of concept that showed beyond a doubt that the <code>timeout</code> method was introducing instability in the very internals of my http client. The finished proof of concept code can look a bit complex, so rather than showing the final concept code straightaway, I&rsquo;ll run you through my thought process instead.</p>

<p>Let&rsquo;s start with the basics and write some code that uses the http client to fetch a random google page. A randomized parameter is added to the google url in order to circumvent any client-side caching. The page fetch itself is wrapped inside a <code>timeout</code> block as we are interested in testing whether the <code>timeout</code> method is corrupting the http client.</p>

<pre><code class="language-ruby"># basics.rb
# timeout doesn't get triggered
require 'curb'
require 'timeout'

timeout(1) do
  Curl.get(&quot;http://www.google.com?foo=#{rand}&quot;)
end
</code></pre>

<p>This code will rarely timeout as a page fetch generally takes way less than one second to complete. This is why we&rsquo;re going to wrap our page fetch inside an infinite while loop.</p>

<pre><code class="language-ruby"># infinite_loop.rb
# timeout gets triggered and Timeout::Error exception gets thrown
require 'curb'
require 'timeout'

timeout(1) do
  while true
    Curl.get(&quot;http://www.google.com?foo=#{rand}&quot;)
  end
end
</code></pre>

<pre><code class="language-bash">$ ruby infinite_loop.rb

/Users/vaneyckt/.rvm/gems/ruby-2.0.0-p594/gems/curb-0.8.8/lib/curl/easy.rb:68:
  in 'perform': execution expired (Timeout::Error)
</code></pre>

<p>The above code is now timing out and throwing a <code>Timeout::Error</code> exception. Next we want to determine whether the timing out of a page fetch could corrupt the internal state of the http client, thereby causing problems for a subsequent page fetch. We&rsquo;ll need to make lots of page fetches to test this, so we&rsquo;re going to wrap all of our current code inside another infinite while loop. Furthermore, we don&rsquo;t want any <code>Timeout::Error</code> exceptions to break us out of this while loop, so we&rsquo;re going to catch and ignore these exceptions inside the while loop we just created. This gives us our finished proof of concept code.</p>

<pre><code class="language-ruby"># proof_of_concept.rb
# timeout corrupts the very internals of the curb http client
require 'curb'
require 'timeout'

while true
  begin
    timeout(1) do
      while true
        Curl.get(&quot;http://www.google.com?foo=#{rand}&quot;)
      end
    end
  rescue Timeout::Error =&gt; e
  end
end
</code></pre>

<pre><code class="language-bash">$ ruby proof_of_concept.rb

/Users/vaneyckt/.rvm/gems/ruby-2.0.0-p594/gems/curb-0.8.8/lib/curl/easy.rb:67:
  in 'add': CURLError: The easy handle is already added to a multi handle
  (Curl::Err::MultiAddedAlready)
</code></pre>

<p>Running the above program will result in an exception being thrown after a few seconds. At some point, the <code>timeout</code> method is causing a <code>Timeout::Error</code> exception to be raised inside a critical code path of the http client. This badly timed <code>Timeout::Error</code> exception leaves the client in an invalid state, which in turn causes the next page fetch to fail with the exception shown above. Hopefully this illustrates why you should avoid creating programs that can have <code>Timeout::Error</code> exceptions pop up <em>absolutely anywhere</em>.</p>

<h3 id="conclusion">Conclusion</h3>

<p>I hope this has convinced you there is nothing you can do to prevent <code>timeout</code> from doing whatever it wants to your program&rsquo;s internal state. There is just no way a program can deal with <code>Timeout::Error</code> exceptions being able to potentially pop up <em>absolutely anywhere</em>. The only time you can really get away with using timeouts is when writing functional code that does not rely on any state. In all other cases, it is best to just avoid timeouts entirely.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/a_javascript_closures_recap/">
                A javascript closures recap
              </a>
            </h1>

            <span class="post-date">Sep 26, 2015</span>

            

            

<p>Javascript closures have always been one those things that I used to navigate by intuition. Recently however, upon stumbling across some code that I did not quite grok, it became clear I should try and obtain a more formal understanding. This post is mainly intended as a quick recap for my future self. It won&rsquo;t go into all the details about closures; instead it will focus on the bits that I found most helpful.</p>

<p>There seem to be very few step-by-step overviews of javascript closures. As a matter of fact, I only found two. Luckily they are both absolute gems. You can find them <a href="http://openhome.cc/eGossip/JavaScript/Closures.html">here</a> and <a href="https://web.archive.org/web/20080209105120/http://blog.morrisjohns.com/javascript_closures_for_dummies">here</a>. I heartily recommend both these articles to anyone wanting to gain a more complete understanding of closures.</p>

<h3 id="closure-basics">Closure basics</h3>

<p>I&rsquo;m going to shamelessly borrow a few lines from the <a href="http://openhome.cc/eGossip/JavaScript/Closures.html">first</a> of the two articles linked above to illustrate the basic concept of a closure.</p>

<pre><code class="language-javascript">function doSome() {
  var x = 10;

  function f(y) {
    return x + y;
  }
  return f;
}

var foo = doSome();
foo(20); // returns 30
foo(30); // returns 40
</code></pre>

<blockquote>
<p>In the above example, the function f creates a closure. If you just look at f, it seems that the variable x is not defined. Actually, x is caught from the enclosing function. A closure is a function which closes (or survives) variables of the enclosing function. In the above example, the function f creates a closure because it closes the variable x into the scope of itself. If the closure object, a Function instance, is still alive, the closed variable x keeps alive. It&rsquo;s like that the scope of the variable x is extended.</p>
</blockquote>

<p>This is really all you need to know about closures: they refer to variables declared outside the scope of the function and by doing so keep these variables alive. Closure behavior can be entirely explained just by keeping these two things in mind.</p>

<h3 id="closures-and-primitive-data-types">Closures and primitive data types</h3>

<p>The rest of this post will go over some code examples to illustrate the behavior of closures for both primitive and object params. In this section, we&rsquo;ll have a look at the behavior of a closure with a primitive data type param.</p>

<h4 id="example-1">Example 1</h4>

<p>The code below will be our starting point for studying closures. Be sure to take a good look at it, as all our examples will be a variation of this code. Throughout this post, we are going to try and understand closures by examining the values returned by the <code>foo()</code> function.</p>

<pre><code class="language-javascript">var prim = 1;

var foo = function(p) {
  var f = function() {
    return p;
  }
  return f;
}(prim);

foo();    // returns 1
prim = 3;
foo();    // returns 1
</code></pre>

<p>When the javascript runtime wants to resolve the value returned by <code>return p;</code>, it finds that this p variable is the same as the p variable from <code>var foo = function(p) {</code>. In other words, there is no direct link between the p from <code>return p;</code> and the variable prim from <code>var prim = 1;</code>. We see this is true because assigning a new value to prim does not cause the value returned by <code>foo()</code> to change.</p>

<h4 id="example-2">Example 2</h4>

<p>Now let&rsquo;s have a look at what happens when we make a small change to the previous code sample by adding the line <code>p = 2;</code> to it.</p>

<pre><code class="language-javascript">var prim = 1;

var foo = function(p) {
  var f = function() {
    return p;
  }
  p = 2;
  return f;
}(prim);

foo();    // returns 2
prim = 3;
foo();    // returns 2
</code></pre>

<p>The code above is interesting in that it shows that the p variable from <code>return p;</code> is indeed the same as the p variable from <code>var foo = function(p) {</code>. Even though the variable f gets created at a time when p is set to 1, the act of setting p to 2 does indeed cause the value returned by <code>foo()</code> to change. This is a great example of a closure keeping a closed variable alive.</p>

<h4 id="example-3">Example 3</h4>

<p>This sample shows code similar to the first, but this time we made the closure close over the prim variable.</p>

<pre><code class="language-javascript">var prim = 1;

var foo = function() {
  return prim;
}

foo();    // returns 1
prim = 3;
foo();    // returns 3
</code></pre>

<p>Here too we can make a similar deduction as we did for the previous samples. When the javascript runtime wants to resolve the value returned by <code>return prim;</code>, it finds that this prim variable is the same as the prim variable from <code>var prim = 1;</code>. This explains why setting prim to 3 causes the value returned by <code>foo()</code> to change.</p>

<h3 id="closures-and-objects">Closures and objects</h3>

<p>In this section we&rsquo;ll see what happens when we take our code samples and change the param from a primitive data type to an object.</p>

<h4 id="example-1-a">Example 1.a</h4>

<p>The code below is interesting because in the previous section we saw that a similar example using a primitive param had both calls to <code>foo()</code> return the same value. So what&rsquo;s different here? Let&rsquo;s inspect how the runtime resolves the variables involved.</p>

<pre><code class="language-javascript">var obj = [&quot;a&quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  return f;
}(obj);

foo();        // returns 1
obj[1] = &quot;b&quot;; // modifies the object pointed to by the obj var
obj[2] = &quot;c&quot;; // modifies the object pointed to by the obj var
foo();        // returns 3
</code></pre>

<p>When the runtime tries to resolve the variable o from <code>return o.length;</code>, it finds that this variable o is the same as the variable o from <code>var foo = function(o) {</code>. We saw this exact same thing in the previous section. Unlike the previous section, the variable o now contains a reference to an array object. This causes our closure to have a direct link to this array object, and thus any changes to it will get reflected in the output of <code>foo()</code>. This explains why the second call to <code>foo()</code> gives a different output than the first.</p>

<p><strong>A good rule of thumb goes like this:</strong></p>

<ul>
<li><strong>if a closed variable contains a value, then the closure links to that variable</strong></li>
<li><strong>if a closed variable contains a reference to an object, then the closure links to that object, and will pick up on any changes made to it</strong></li>
</ul>

<h4 id="example-1-b">Example 1.b</h4>

<p>Note that the closure will only pick up on changes made to the particular object that was present when the closure was created. Assigning a new object to the obj variable after the closure was created will have no effect. The code below illustrates this.</p>

<pre><code class="language-javascript">var obj = [&quot;a&quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  return f;
}(obj);

foo();                 // returns 1
obj = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]; // assign a new array object to the obj variable
foo();                 // returns 1
</code></pre>

<p>In fact, this code is practically identical to the code from Example 1 of the previous section.</p>

<h4 id="example-2-1">Example 2</h4>

<p>We&rsquo;ll now modify the previous code sample a bit. This time we&rsquo;ll take a look at what happens when we add the line <code>o[1] = &quot;b&quot;;</code>.</p>

<pre><code class="language-javascript">var obj = [&quot;a&quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  o[1] = &quot;b&quot;;
  return f;
}(obj);

foo();        // returns 2
obj[1] = &quot;b&quot;;
obj[2] = &quot;c&quot;;
foo();        // returns 3
</code></pre>

<p>Once again, we can start by reasoning about how the runtime resolves the variable o from <code>return o.length;</code>. As you probably know by now, this variable o is the same as the variable o from <code>var foo = function(o) {</code>. And since it contains a reference to an object, any changes to this object will get reflected in the output of <code>foo()</code>. This explains why the first call to <code>foo()</code> now returns 2, whereas previously it was returning 1.</p>

<h4 id="example-3-1">Example 3</h4>

<p>If you managed to make it this far, this last bit of code should hold no surprises for you.</p>

<pre><code class="language-javascript">var obj = [&quot;a&quot;];

var foo = function() {
  return obj.length;
}

foo();        // returns 1
obj[1] = &quot;b&quot;;
obj[2] = &quot;c&quot;;
foo();        // returns 3
</code></pre>

<p>The runtime will resolve the variable obj from <code>return obj.length;</code> to be the same as the variable obj from <code>var obj = [&quot;a&quot;];</code>. As a result, any changes to the obj variable will have an effect on the output of <code>foo()</code>.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Hopefully this post has demystified closures a bit. Time and time again, we&rsquo;ve shown how following a few simple steps will lead you to understand their behavior. Just keep in mind these rules of thumb and you should be good to go:</p>

<ul>
<li>if a closed variable contains a value, then the closure links to that variable</li>
<li>if a closed variable contains a reference to an object, then the closure links to that object, and will pick up on any changes made to it</li>
</ul>

<p>Ideally, this is going to become my go-to post for providing an introduction to closures. So please let me know any suggestions you might have to improve this post.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/understanding_iostat/">
                Understanding iostat
              </a>
            </h1>

            <span class="post-date">Aug 24, 2015</span>

            

            

<p>I&rsquo;ve been spending a lot of time lately looking at I/O performance and reading up about the <code>iostat</code> command. While this command provides a wealth of I/O performance data, the sheer amount of it all can make it hard to see the forest for the trees. In this post, we&rsquo;ll talk about interpreting this data. Before we continue, I would first like to thank the authors of the blog posts mentioned below, as each of these has helped me understand <code>iostat</code> and its many complexities just a little bit better.</p>

<ul>
<li><a href="http://www.thattommyhall.com/2011/02/18/iops-linux-iostat/">Measuring disk usage in linux (%iowait vs IOPS)</a></li>
<li><a href="http://www.pythian.com/blog/basic-io-monitoring-on-linux/">Basic I/O monitoring on linux</a></li>
<li><a href="http://brooker.co.za/blog/2014/07/04/iostat-pct.html">Two traps in iostat: %util and svctm</a></li>
<li><a href="https://blog.pregos.info/wp-content/uploads/2010/09/iowait.txt">What exactly is &ldquo;iowait&rdquo;?</a></li>
<li><a href="https://www.igvita.com/2009/06/23/measuring-optimizing-io-performance/">Measuring &amp; optimizing I/O performance</a></li>
<li><a href="http://dom.as/2009/03/11/iostat/">Iostat</a></li>
<li><a href="http://www.xaprb.com/blog/2010/09/06/beware-of-svctm-in-linuxs-iostat/">Beware of svctm in linux&rsquo;s iostat</a></li>
<li><a href="http://www.psce.com/blog/2012/04/18/analyzing-io-performance/">Analyzing I/O performance</a></li>
</ul>

<p>The <code>iostat</code> command can display both basic and extended metrics. We&rsquo;ll take a look at the basic metrics first before moving on to extended metrics in the remainder of this post. Note that this post will not go into detail about every last metric. Instead, I have decided to focus on just those metrics that I found to be especially useful, as well as those that seem to be often misunderstood.</p>

<h3 id="basic-iostat-metrics">Basic iostat metrics</h3>

<p>The <code>iostat</code> command lists basic metrics by default. The <code>-m</code> parameter causes metrics to be displayed in megabytes per second instead of blocks or kilobytes per second. Using the <code>5</code> parameter causes <code>iostat</code> to recalculate metrics every 5 seconds, thereby making the numbers an average over this interval.</p>

<pre><code class="language-bash">$ iostat -m 5

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           8.84    0.16    3.91    7.73    0.04   79.33

Device:            tps    MB_read/s    MB_wrtn/s    MB_read    MB_wrtn
xvdap1           46.34         0.33         1.03    2697023    8471177
xvdb              0.39         0.00         0.01       9496      71349
xvdg             65.98         1.34         0.97   11088426    8010609
xvdf            205.17         1.62         2.68   13341297   22076001
xvdh             51.16         0.64         1.43    5301463   11806257
</code></pre>

<p>The <code>tps</code> number here is the number of I/O Operations Per Second (IOPS). Wikipedia has <a href="https://en.wikipedia.org/wiki/IOPS#Examples">a nice list of average IOPS for different storage devices</a>. This should give you a pretty good idea of the I/O load on your machine.</p>

<p>Some people put a lot of faith in the <code>%iowait</code> metric as an indicator of I/O performance. However, <code>%iowait</code> is first and foremost a CPU metric that measures the percentage of time the CPU is idle while waiting for an I/O operation to complete. This metric is heavily influenced by both your CPU speed and CPU load and is therefore easily misinterpreted.</p>

<p>For example, consider a system with just two processes: the first one heavily I/O intensive, the second one heavily CPU intensive. As the second process will prevent the CPU from going idle, the <code>%iowait</code> metric will stay low despite the first process&rsquo;s high I/O utilization. Other examples illustrating the deceptive nature of <code>%iowait</code> can be found <a href="https://blog.pregos.info/wp-content/uploads/2010/09/iowait.txt">here</a> (<a href="https://gist.github.com/vaneyckt/58028fb0ddbdbf561e60">mirror</a>). The only thing <code>%iowait</code> really tells us is that the CPU occasionally idles while there is an outstanding I/O request, and could thus be made to handle more computational work.</p>

<h3 id="extended-iostat-metrics">Extended iostat metrics</h3>

<p>Let&rsquo;s now take a look at the extended metrics by calling the <code>iostat -x</code> command.</p>

<pre><code class="language-bash">$ iostat -mx 5

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           8.84    0.16    3.91    7.73    0.04   79.33

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvdap1            0.57     6.38   20.85   25.49     0.33     1.03    59.86     0.27   17.06   13.15   20.25   1.15   5.33
xvdb              0.00     1.93    0.10    0.29     0.00     0.01    51.06     0.00    7.17    0.33    9.66   0.09   0.00
xvdg              0.55     4.69   42.04   23.94     1.34     0.97    71.89     0.44    6.63    6.82    6.28   1.16   7.67
xvdf              7.33    41.35  132.66   72.52     1.62     2.68    42.87     0.49    2.37    2.79    1.59   0.36   7.42
xvdh              0.00     4.54   15.54   35.63     0.64     1.43    83.04     0.00   10.22    8.39   11.02   1.30   6.68
</code></pre>

<p>The <code>r/s</code> and <code>w/s</code> numbers show the amount of read and write requests issued to the I/O device per second. These numbers provide a more detailed breakdown of the <code>tps</code> metric we saw earlier, as <code>tps = r/s + w/s</code>.</p>

<p>The <code>avgqu-sz</code> metric is an important value. Its name is rather poorly chosen as it doesn&rsquo;t actually show the number of operations that are queued but not yet serviced. Instead, it shows <a href="http://www.xaprb.com/blog/2010/01/09/how-linux-iostat-computes-its-results">the number of operations that were either queued, or being serviced</a>. Ideally, you&rsquo;d want to have an idea of this value during normal operations for use as a baseline number for when trouble occurs. Single digit numbers with the occasional double digit spike are safe(ish) values. Triple digit numbers generally are not.</p>

<p>The <code>await</code> metric is the average time from when a request was put in the queue to when the request was completed. This is the sum of the time a request was waiting in the queue and the time our storage device was working on servicing the request. This metric is highly dependent on the number of items in the queue. Much like <code>avgqu-sz</code>, you&rsquo;ll want to have an idea of the value of this metric during normal operations for use as a baseline.</p>

<p>Our next metric is <code>svctm</code>. You&rsquo;ll find a lot of older blog posts that go into quite some detail about this one. However, <code>man iostat</code> makes it quite clear that this metric has since been deprecated and should no longer be trusted.</p>

<p>Our last metric is <code>%util</code>. Just like <code>svctm</code>, this metric has been touched by the progress of technology as well. The <code>man iostat</code> pages contain the information shown below.</p>

<blockquote>
<p><strong>%util</strong></p>

<p>Percentage of elapsed time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100% for devices serving requests serially. But for devices serving requests in parallel, such as RAID arrays and modern SSDs, this number does not reflect their performance limits.</p>
</blockquote>

<p>Its common to assume that the closer a device gets to 100% utilization, the more saturated it becomes. This is true when the storage device corresponds to a single magnetic disk as such a device can only serve one request at a time. However, a single SSD or a RAID array consisting of multiple disks can serve multiple requests simultaneously. For such devices, <code>%util</code> essentially indicates the percentage of time that the device was busy serving one or more requests. Unfortunately, this value tells us absolutely nothing about the maximum number of simultaneous requests such a device can handle. This metric should therefore not be treated as a saturation indicator for either SSDs or RAID arrays.</p>

<h3 id="conclusion">Conclusion</h3>

<p>By now it should be clear that <code>iostat</code> is an incredibly powerful tool, the metrics of which can take some experience to interpret correctly. In a perfect world your machines should regularly be writing these metrics to a monitoring service, so you&rsquo;ll always have access to good baseline numbers. In an imperfect world, just knowing your baseline IOPS values will already go a long way when trying to diagnose whether a slowdown is I/O related.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/">
                Safer bash scripts with &#39;set -euxo pipefail&#39;
              </a>
            </h1>

            <span class="post-date">Mar 16, 2015</span>

            

            

<p>Often times developers go about writing bash scripts the same as writing code in a higher-level language. This is a big mistake as higher-level languages offer safeguards that are not present in bash scripts by default. For example, a Ruby script will throw an error when trying to read from an uninitialized variable, whereas a bash script won&rsquo;t. In this article, we&rsquo;ll look at how we can improve on this.</p>

<p>The bash shell comes with several builtin commands for modifying the behavior of the shell itself. We are particularly interested in the <a href="https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html">set builtin</a>, as this command has several options that will help us write safer scripts. I hope to convince you that it&rsquo;s a really good idea to add <code>set -euxo pipefail</code> to the beginning of all your future bash scripts.</p>

<h3 id="set-e">set -e</h3>

<p>The <code>-e</code> option will cause a bash script to exit immediately when a command fails. This is generally a vast improvement upon the default behavior where the script just ignores the failing command and continues with the next line. This option is also smart enough to not react on failing commands that are part of conditional statements. Moreover, you can append a command with <code>|| true</code> for those rare cases where you don&rsquo;t want a failing command to trigger an immediate exit.</p>

<h4 id="before">Before</h4>

<pre><code class="language-bash">#!/bin/bash

# 'foo' is a non-existing command
foo
echo &quot;bar&quot;

# output
# ------
# line 4: foo: command not found
# bar
#
# Note how the script didn't exit when the foo command could not be found.
# Instead it continued on and echoed 'bar'.
</code></pre>

<h4 id="after">After</h4>

<pre><code class="language-bash">#!/bin/bash
set -e

# 'foo' is a non-existing command
foo
echo &quot;bar&quot;

# output
# ------
# line 5: foo: command not found
#
# This time around the script exited immediately when the foo command wasn't found.
# Such behavior is much more in line with that of higher-level languages.
</code></pre>

<h4 id="any-command-returning-a-non-zero-exit-code-will-cause-an-immediate-exit">Any command returning a non-zero exit code will cause an immediate exit</h4>

<pre><code class="language-bash">#!/bin/bash
set -e

# 'ls' is an existing command, but giving it a nonsensical param will cause
# it to exit with exit code 1
$(ls foobar)
echo &quot;bar&quot;

# output
# ------
# ls: foobar: No such file or directory
#
# I'm putting this in here to illustrate that it's not just non-existing commands
# that will cause an immediate exit.
</code></pre>

<h4 id="preventing-an-immediate-exit">Preventing an immediate exit</h4>

<pre><code class="language-bash">#!/bin/bash
set -e

foo || true
$(ls foobar) || true
echo &quot;bar&quot;

# output
# ------
# line 4: foo: command not found
# ls: foobar: No such file or directory
# bar
#
# Sometimes we want to ensure that, even when 'set -e' is used, the failure of
# a particular command does not cause an immediate exit. We can use '|| true' for this.
</code></pre>

<h4 id="failing-commands-in-a-conditional-statement-will-not-cause-an-immediate-exit">Failing commands in a conditional statement will not cause an immediate exit</h4>

<pre><code class="language-bash">#!/bin/bash
set -e

# we make 'ls' exit with exit code 1 by giving it a nonsensical param
if ls foobar; then
  echo &quot;foo&quot;
else
  echo &quot;bar&quot;
fi

# output
# ------
# ls: foobar: No such file or directory
# bar
#
# Note that 'ls foobar' did not cause an immediate exit despite exiting with
# exit code 1. This is because the command was evaluated as part of a
# conditional statement.
</code></pre>

<p>That&rsquo;s all for <code>set -e</code>. However, <code>set -e</code> by itself is far from enough. We can further improve upon the behavior created by <code>set -e</code> by combining it with <code>set -o pipefail</code>. Let&rsquo;s have a look at that next.</p>

<h3 id="set-o-pipefail">set -o pipefail</h3>

<p>The bash shell normally only looks at the exit code of the last command of a pipeline. This behavior is not ideal as it causes the <code>-e</code> option to only be able to act on the exit code of a pipeline&rsquo;s last command. This is where <code>-o pipefail</code> comes in. This particular option sets the exit code of a pipeline to that of the rightmost command to exit with a non-zero status, or to zero if all commands of the pipeline exit successfully.</p>

<h4 id="before-1">Before</h4>

<pre><code class="language-bash">#!/bin/bash
set -e

# 'foo' is a non-existing command
foo | echo &quot;a&quot;
echo &quot;bar&quot;

# output
# ------
# a
# line 5: foo: command not found
# bar
#
# Note how the non-existing foo command does not cause an immediate exit, as
# it's non-zero exit code is ignored by piping it with '| echo &quot;a&quot;'.
</code></pre>

<h4 id="after-1">After</h4>

<pre><code class="language-bash">#!/bin/bash
set -eo pipefail

# 'foo' is a non-existing command
foo | echo &quot;a&quot;
echo &quot;bar&quot;

# output
# ------
# a
# line 5: foo: command not found
#
# This time around the non-existing foo command causes an immediate exit, as
# '-o pipefail' will prevent piping from causing non-zero exit codes to be ignored.
</code></pre>

<p>This section hopefully made it clear that <code>-o pipefail</code> provides an important improvement upon just using <code>-e</code> by itself. However, as we shall see in the next section, we can still do more to make our scripts behave like higher-level languages.</p>

<h3 id="set-u">set -u</h3>

<p>This option causes the bash shell to treat unset variables as an error and exit immediately. Unset variables are a common cause of bugs in shell scripts, so having unset variables cause an immediate exit is often highly desirable behavior.</p>

<h4 id="before-2">Before</h4>

<pre><code class="language-bash">#!/bin/bash
set -eo pipefail

echo $a
echo &quot;bar&quot;

# output
# ------
#
# bar
#
# The default behavior will not cause unset variables to trigger an immediate exit.
# In this particular example, echoing the non-existing $a variable will just cause
# an empty line to be printed.
</code></pre>

<h4 id="after-2">After</h4>

<pre><code class="language-bash">#!/bin/bash
set -euo pipefail

echo &quot;$a&quot;
echo &quot;bar&quot;

# output
# ------
# line 5: a: unbound variable
#
# Notice how 'bar' no longer gets printed. We can clearly see that '-u' did indeed
# cause an immediate exit upon encountering an unset variable.
</code></pre>

<h4 id="dealing-with-a-b-variable-assignments">Dealing with ${a:-b} variable assignments</h4>

<p>Sometimes you&rsquo;ll want to use a <a href="https://unix.stackexchange.com/questions/122845/using-a-b-for-variable-assignment-in-scripts/122878">${a:-b} variable assignment</a> to ensure a variable is assigned a default value of <code>b</code> when <code>a</code> is either empty or undefined. The <code>-u</code> option is smart enough to not cause an immediate exit in such a scenario.</p>

<pre><code class="language-bash">#!/bin/bash
set -euo pipefail

DEFAULT=5
RESULT=${VAR:-$DEFAULT}
echo &quot;$RESULT&quot;

# output
# ------
# 5
#
# Even though VAR was not defined, the '-u' option realizes there's no need to cause
# an immediate exit in this scenario as a default value has been provided.
</code></pre>

<h4 id="using-conditional-statements-that-check-if-variables-are-set">Using conditional statements that check if variables are set</h4>

<p>Sometimes you want your script to not immediately exit when an unset variable is encountered. A common example is checking for a variable&rsquo;s existence inside an <code>if</code> statement.</p>

<pre><code class="language-bash">#!/bin/bash
set -euo pipefail

if [ -z &quot;${MY_VAR:-}&quot; ]; then
  echo &quot;MY_VAR was not set&quot;
fi

# output
# ------
# MY_VAR was not set
#
# In this scenario we don't want our program to exit when the unset MY_VAR variable
# is evaluated. We can prevent such an exit by using the same syntax as we did in the
# previous example, but this time around we specify no default value.
</code></pre>

<p>This section has brought us a lot closer to making our bash shell behave like higher-level languages. While <code>-euo pipefail</code> is great for the early detection of all kinds of problems, sometimes it won&rsquo;t be enough. This is why in the next section we&rsquo;ll look at an option that will help us figure out those really tricky bugs that you encounter every once in a while.</p>

<h3 id="set-x">set -x</h3>

<p>The <code>-x</code> option causes bash to print each command before executing it. This can be a great help when trying to debug a bash script failure. Note that arguments get expanded before a command gets printed, which will cause our logs to contain the actual argument values that were present at the time of execution!</p>

<pre><code class="language-bash">#!/bin/bash
set -euxo pipefail

a=5
echo $a
echo &quot;bar&quot;

# output
# ------
# + a=5
# + echo 5
# 5
# + echo bar
# bar
</code></pre>

<p>That&rsquo;s it for the <code>-x</code> option. It&rsquo;s pretty straightforward, but can be a great help for debugging. Next up, we&rsquo;ll look at an option I had never heard of before that was suggested by a reader of this blog.</p>

<h3 id="reader-suggestion-set-e">Reader suggestion: set -E</h3>

<p><a href="http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html">Traps</a> are pieces of code that fire when a bash script catches certain signals. Aside from the usual signals (e.g. <code>SIGINT</code>, <code>SIGTERM</code>, &hellip;), traps can also be used to catch special bash signals like <code>EXIT</code>, <code>DEBUG</code>, <code>RETURN</code>, and <code>ERR</code>. However, reader Kevin Gibbs pointed out that using <code>-e</code> without <code>-E</code> will cause an <code>ERR</code> trap to not fire in certain scenarios.</p>

<h4 id="before-3">Before</h4>

<pre><code class="language-bash">#!/bin/bash
set -euo pipefail

trap &quot;echo ERR trap fired!&quot; ERR

myfunc()
{
  # 'foo' is a non-existing command
  foo
}

myfunc
echo &quot;bar&quot;

# output
# ------
# line 9: foo: command not found
#
# Notice that while '-e' did indeed cause an immediate exit upon trying to execute
# the non-existing foo command, it did not case the ERR trap to be fired.
</code></pre>

<h4 id="after-3">After</h4>

<pre><code class="language-bash">#!/bin/bash
set -Eeuo pipefail

trap &quot;echo ERR trap fired!&quot; ERR

myfunc()
{
  # 'foo' is a non-existing command
  foo
}

myfunc
echo &quot;bar&quot;

# output
# ------
# line 9: foo: command not found
# ERR trap fired!
#
# Not only do we still have an immediate exit, we can also clearly see that the
# ERR trap was actually fired now.
</code></pre>

<p>The documentation states that <code>-E</code> needs to be set if we want the <code>ERR</code> trap to be inherited by shell functions, command substitutions, and commands that are executed in a subshell environment. The <code>ERR</code> trap is normally not inherited in such cases.</p>

<h3 id="conclusion">Conclusion</h3>

<p>I hope this post showed you why using <code>set -euxo pipefail</code> (or <code>set -Eeuxo pipefail</code>) is such a good idea. If you have any other options you want to suggest, then please let me know and I&rsquo;ll be happy to add them to this list.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/an_introduction_to_javascript_promises/">
                An introduction to javascript promises
              </a>
            </h1>

            <span class="post-date">Feb 7, 2015</span>

            

            

<p>I recently had to write some javascript code that required the sequential execution of half a dozen asynchronous requests. I figured this was the perfect time to learn a bit more about javascript promises. This post is a recap of what I read in these <a href="http://www.html5rocks.com/en/tutorials/es6/promises/">three</a> <a href="http://www.mullie.eu/how-javascript-promises-work/">amazing</a> <a href="http://www.sitepoint.com/overview-javascript-promises/">write-ups</a>.</p>

<h3 id="what-are-promises">What are promises?</h3>

<p>A Promise object represents a value that may not be available yet, but will be resolved at some point in future. This abstraction allows you to write asynchronous code in a more synchronous fashion. For example, you can use a Promise object to represent data that will eventually be returned by a call to a remote web service. The <code>then</code> and <code>catch</code> methods can be used to attach callbacks that will be triggered once the data arrives. We&rsquo;ll take a closer look at these two methods in the next sections. For now, let&rsquo;s write a simple AJAX request example that prints a random joke.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &quot;http://api.icndb.com/jokes/random&quot;,
    success: function(result) {
      resolve(result[&quot;value&quot;][&quot;joke&quot;]);
    }
  });
});

promise.then(function(result) {
  console.log(result);
});
</code></pre>

<p>Note how the Promise object is just a wrapper around the AJAX request and how we&rsquo;ve instructed the <code>success</code> callback to trigger the <code>resolve</code> method. We&rsquo;ve also attached a callback to our Promise object with the <code>then</code> method. This callback gets triggered when the <code>resolve</code> method gets called. The <code>result</code> variable of this callback will contain the data that was passed to the <code>resolve</code> method.</p>

<p>Before we take a closer look at the <code>resolve</code> method, let&rsquo;s first investigate the Promise object a bit more. A Promise object can have one of three states:</p>

<ul>
<li><strong>fulfilled</strong> - the action relating to the Promise succeeded</li>
<li><strong>rejected</strong> - the action relating to the Promise failed</li>
<li><strong>pending</strong> - the Promise hasn&rsquo;t been fulfilled or rejected yet</li>
</ul>

<p>A pending Promise object can be fulfilled or rejected by calling <code>resolve</code> or <code>reject</code> on it. Once a Promise is fulfilled or rejected, this state gets permanently associated with it. The state of a fulfilled Promise also includes the data that was passed to <code>resolve</code>, just as the state of a rejected Promise also includes the data that was passed to <code>reject</code>. In summary, we can say that a Promise executes only once and stores the result of its execution.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &quot;http://api.icndb.com/jokes/random&quot;,
    success: function(result) {
      resolve(result[&quot;value&quot;][&quot;joke&quot;]);
    }
  });
});

promise.then(function(result) {
  console.log(result);
});

promise.then(function(result) {
  console.log(result);
});
</code></pre>

<p>We can test whether a Promise only ever executes once by adding a second callback to the previous example. In this case, we see that only one AJAX request gets made and that the same joke gets printed to the console twice. This clearly shows that our Promise was only executed once.</p>

<h3 id="the-then-method-and-chaining">The <code>then</code> method and chaining</h3>

<p>The <code>then</code> method takes two arguments: a mandatory success callback and an optional failure callback. These callbacks are called when the Promise is settled (i.e. either fulfilled or rejected). If the Promise was fulfilled, the success callback will be fired with the data you passed to <code>resolve</code>. If the Promise was rejected, the failure callback will be called with the data you passed to <code>reject</code>. We&rsquo;ve already covered most of this in the previous section.</p>

<p>The real magic with the <code>then</code> method happens when you start chaining several of them together. This chaining allows you to express your logic in separate stages, each of which can be made responsible for transforming data passed on by the previous stage or for running additional asynchronous requests. The code below shows how data returned by the success callback of the first <code>then</code> method becomes available to the success callback of the second <code>then</code> method.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &quot;http://api.icndb.com/jokes/random&quot;,
    success: function(result) {
      resolve(result[&quot;value&quot;][&quot;joke&quot;]);
    }
  });
});

promise.then(function(result) {
  return result;
}).then(function(result) {
  console.log(result);
});
</code></pre>

<p>This chaining is possible because the <code>then</code> method returns a new Promise object that will resolve to the return value of the callback. Or in other words, by calling <code>return result;</code> we cause the creation of an anonymous Promise object that looks something like shown below. Notice that this particular anonymous Promise object will resolve immediately, as it does not make any asynchronous requests.</p>

<pre><code class="language-javascript">new Promise(function(resolve, reject) {
  resolve(result);
});
</code></pre>

<p>Now that we understand that the <code>then</code> method always returns a Promise object, let&rsquo;s take a look at what happens when we tell the callback of a <code>then</code> method to explicitly return a Promise object.</p>

<pre><code class="language-javascript">function getJokePromise() {
  return new Promise(function(resolve, reject) {
    $.ajax({
      url: &quot;http://api.icndb.com/jokes/random&quot;,
      success: function(result) {
        resolve(result[&quot;value&quot;][&quot;joke&quot;]);
      }
    });
  });
}

getJokePromise().then(function(result) {
  console.log(result);
  return getJokePromise();
}).then(function(result) {
  console.log(result);
});
</code></pre>

<p>In this case, we end up sequentially executing two asynchronous requests. When the first Promise is resolved, the first joke is printed and a new Promise object is returned by the <code>then</code> method. This new Promise object then has <code>then</code> called on it. When the Promise succeeds, the <code>then</code> success callback is triggered and the second joke is printed.</p>

<p>The takeaway from all this is that calling <code>return</code> in a <code>then</code> callback will always result in returning a Promise object. It is this that allows for <code>then</code> chaining!</p>

<h3 id="error-handling">Error handling</h3>

<p>We mentioned in the previous section how the <code>then</code> method can take an optional failure callback that gets triggered when <code>reject</code> is called. It is customary to reject with an Error object as they capture a stack trace, thereby facilitating debugging.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &quot;http://random.url.com&quot;,
    success: function(result) {
      resolve(result[&quot;value&quot;][&quot;joke&quot;]);
    },
    error: function(jqxhr, textStatus) {
      reject(Error(&quot;The AJAX request failed.&quot;));
    }
  });
});

promise.then(function(result) {
  console.log(result);
}, function(error) {
  console.log(error);
  console.log(error.stack);
});
</code></pre>

<p>Personally, I find this a bit hard to read. Luckily we can use the <code>catch</code> method to make this look a bit nicer. There&rsquo;s nothing special about the <code>catch</code> method. In fact, it&rsquo;s just sugar for <code>then(undefined, func)</code>, but it definitely makes code easier to read.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &quot;http://random.url.com&quot;,
    success: function(result) {
      resolve(result[&quot;value&quot;][&quot;joke&quot;]);
    },
    error: function(jqxhr, textStatus) {
      reject(Error(&quot;The AJAX request failed.&quot;));
    }
  });
});

promise.then(function(result) {
  console.log(result);
}).then(function(result) {
  console.log(&quot;foo&quot;); // gets skipped
}).then(function(result) {
  console.log(&quot;bar&quot;); // gets skipped
}).catch(function(error) {
  console.log(error);
  console.log(error.stack);
});
</code></pre>

<p>Aside from illustrating improved readability, the above code showcases another aspect of the <code>reject</code> method in that Promise rejections will cause your code to skip forward to the next <code>then</code> method that has a rejection callback (or the next <code>catch</code> method, since this is equivalent). It is this fallthrough behavior that causes this code to not print &ldquo;foo&rdquo; or &ldquo;bar&rdquo;!</p>

<p>As a final point, it is useful to know that a Promise is implicitly rejected if an error is thrown in its constructor callback. This means it&rsquo;s useful to do all your Promise related work inside the Promise constructor callback, so errors automatically become rejections.</p>

<pre><code class="language-javascript">var promise = new Promise(function(resolve, reject) {
  // JSON.parse throws an error if you feed it some
  // invalid JSON, so this implicitly rejects
  JSON.parse(&quot;This ain't JSON&quot;);
});

promise.then(function(result) {
  console.log(result);
}).catch(function(error) {
  console.log(error);
});
</code></pre>

<p>The above code will cause the Promise to be rejected and an error to be printed because it will fail to parse the invalid JSON string.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/unwanted_spot_instance_termination_in_multi_az_asg/">
                Unwanted spot instance termination in multi-AZ ASG
              </a>
            </h1>

            <span class="post-date">Jan 24, 2015</span>

            

            <p>An auto scaling group is an AWS abstraction that facilitates increasing or decreasing the number of EC2 instances within your application&rsquo;s architecture. Spot instances are unused AWS servers that are auctioned off for little money. The combination of these two allows for large auto scaling groups at low costs. However, you can lose your spot instances at a moment&rsquo;s notice as soon as someone out there wants to pay more than you do.</p>

<p>Knowing all this, I recently found myself looking into why AWS was terminating several of our spot instances every day. We were bidding 20% over the average price, so it seemed unlikely that this was being caused by a monetary issue. Nevertheless, we kept noticing multiple spot instances disappearing on a daily basis.</p>

<p>It took a while to get to the bottom of things, but it turned out that this particular problem was being caused by an unfortunate combination of:</p>

<ul>
<li>our auto scaling group spanning multiple availability zones</li>
<li>our scaling code making calls to <code>TerminateInstanceInAutoScalingGroup</code></li>
</ul>

<p>The step-by-step explanation of this issue was as follows:</p>

<ul>
<li>our scaling code was asking AWS to put 10 instances in our auto scaling group</li>
<li>AWS obliged and put 5 instances in availability zone A and another 5 in zone B</li>
<li>some time later our scaling code would decide that 2 specific instances were no longer needed. A call would be made to <code>TerminateInstanceInAutoScalingGroup</code> to have just these 2 specific instances terminated.</li>
<li>if these 2 instances happened to be in the same availability zone, then one zone would now have 3 instances, while the other one would now have 5</li>
<li>AWS would detect that both zones were no longer balanced and would initiate a rebalancing action. This rebalancing action would terminate one of the instances in the zone with 5 instances, and spin up another instance in the zone with 3 instances.</li>
</ul>

<p>So while this action did indeed end up rebalancing the instances across the different availability zones, it also inadvertently ended up terminating a running instance.</p>

<p>The relevant entry from the <a href="http://awsdocs.s3.amazonaws.com/AutoScaling/latest/as-dg.pdf">AWS Auto Scaling docs</a> is shown below.</p>

<blockquote>
<p><strong>Instance Distribution and Balance across Multiple Zones</strong></p>

<p>Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. Auto Scaling attempts to launch new instances in the Availability Zone with the fewest instances. If the attempt fails, however, Auto Scaling will attempt to launch in other zones until it succeeds.</p>

<p>Certain operations and conditions can cause your Auto Scaling group to become unbalanced. Auto Scaling compensates by creating a rebalancing activity under any of the following conditions:</p>

<ul>
<li>You issue a request to change the Availability Zones for your group.</li>
<li>You call <code>TerminateInstanceInAutoScalingGroup</code>, which causes the group to become unbalanced.</li>
<li>An Availability Zone that previously had insufficient capacity recovers and has additional capacity available.</li>
</ul>

<p>Auto Scaling always launches new instances before attempting to terminate old ones, so a rebalancing activity will not compromise the performance or availability of your application.</p>

<p><strong>Multi-Zone Instance Counts when Approaching Capacity</strong></p>

<p>Because Auto Scaling always attempts to launch new instances before terminating old ones, being at or near the specified maximum capacity could impede or completely halt rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin during a rebalancing activity (or by a 1-instance margin, whichever is greater). The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either as a result of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the grouptypically a few minutes.</p>
</blockquote>

<p>I&rsquo;m not sure about the best way to deal with this behavior. In our case, we just restricted our auto scaling group to one availability zone. This was good enough for us as none of the work done by our spot instances is critical. Going through the <a href="http://awsdocs.s3.amazonaws.com/AutoScaling/latest/as-dg.pdf">docs</a>, it seems one approach might be to disable the <code>AZRebalance</code> process. However, I have not had the chance to try this, so I cannot guarantee a lack of unexpected side effects.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/creating_an_ec2_instance_in_a_vpc_with_the_aws_cli/">
                Creating an EC2 Instance in a VPC with the AWS CLI
              </a>
            </h1>

            <span class="post-date">Oct 29, 2014</span>

            

            

<p>Setting up an EC2 instance on AWS used to be as straightforward as provisioning a machine and SSHing into it. However, this process has become a bit more complicated now that Amazon VPC has become the standard for managing machines in the cloud.</p>

<p>So what exactly is a Virtual Private Cloud? Amazon defines a VPC as &lsquo;a logically isolated section of the AWS Cloud&rsquo;. Instances inside a VPC can by default only communicate with other instances in the same VPC and are therefore invisible to the rest of the internet. This means they will not accept SSH connections coming from your computer, nor will they respond to any http requests. In this article we&rsquo;ll look into changing these default settings into something more befitting a general purpose server.</p>

<h3 id="setting-up-your-vpc">Setting up your VPC</h3>

<p>Start by installing the <a href="http://aws.amazon.com/cli">AWS Command Line Interface</a> on your machine if you haven&rsquo;t done so already. With this done, we can now create our VPC.</p>

<pre><code class="language-bash">$ vpcId=`aws ec2 create-vpc --cidr-block 10.0.0.0/28 --query 'Vpc.VpcId' --output text`
</code></pre>

<p>There are several interesting things here:</p>

<ul>
<li>the <code>--cidr-block</code> parameter specifies a /28 netmask that allows for 16 IP addresses. This is the smallest supported netmask.</li>
<li>the <code>create-vpc</code> command returns a JSON string. We can filter out specific fields from this string by using the <code>--query</code> and <code>--output</code> parameters.</li>
</ul>

<p>The next step is to overwrite the default VPC DNS settings. As mentioned earlier, instances launched inside a VPC are invisible to the rest of the internet by default. AWS therefore does not bother assigning them a public DNS name. Luckily this can be changed easily.</p>

<pre><code class="language-bash">$ aws ec2 modify-vpc-attribute --vpc-id $vpcId --enable-dns-support &quot;{\&quot;Value\&quot;:true}&quot;
$ aws ec2 modify-vpc-attribute --vpc-id $vpcId --enable-dns-hostnames &quot;{\&quot;Value\&quot;:true}&quot;
</code></pre>

<h3 id="adding-an-internet-gateway">Adding an Internet Gateway</h3>

<p>Next we need to connect our VPC to the rest of the internet by attaching an internet gateway. Our VPC would be isolated from the internet without this.</p>

<pre><code class="language-bash">$ internetGatewayId=`aws ec2 create-internet-gateway --query 'InternetGateway.InternetGatewayId' --output text`
$ aws ec2 attach-internet-gateway --internet-gateway-id $internetGatewayId --vpc-id $vpcId
</code></pre>

<h3 id="creating-a-subnet">Creating a Subnet</h3>

<p>A VPC can have multiple subnets. Since our use case only requires one, we can reuse the cidr-block specified during VPC creation so as to get a single subnet that spans the entire VPC address space.</p>

<pre><code class="language-bash">$ subnetId=`aws ec2 create-subnet --vpc-id $vpcId --cidr-block 10.0.0.0/28 --query 'Subnet.SubnetId' --output text`
</code></pre>

<p>While this <code>--cidr-block</code> parameter specifies a subnet that can contain 16 IP addresses (10.0.0.1 - 10.0.0.16), AWS will reserve 5 of those for private use. While this doesn&rsquo;t really have an impact on our use case, it is still good to be aware of such things.</p>

<h3 id="configuring-the-route-table">Configuring the Route Table</h3>

<p>Each subnet needs to have a route table associated with it to specify the routing of its outbound traffic. By default every subnet inherits the default VPC route table which allows for intra-VPC communication only.</p>

<p>Here we add a route table to our subnet so as to allow traffic not meant for an instance inside the VPC to be routed to the internet through the internet gateway we created earlier.</p>

<pre><code class="language-bash">$ routeTableId=`aws ec2 create-route-table --vpc-id $vpcId --query 'RouteTable.RouteTableId' --output text`
$ aws ec2 associate-route-table --route-table-id $routeTableId --subnet-id $subnetId
$ aws ec2 create-route --route-table-id $routeTableId --destination-cidr-block 0.0.0.0/0 --gateway-id $internetGatewayId
</code></pre>

<h3 id="adding-a-security-group">Adding a Security Group</h3>

<p>Before we can launch an instance, we first need to create a security group that specifies which ports should allow traffic. For now we&rsquo;ll just allow anyone to try and make an SSH connection by opening port 22 to any IP address.</p>

<pre><code class="language-bash">$ securityGroupId=`aws ec2 create-security-group --group-name my-security-group --description &quot;my-security-group&quot; --vpc-id $vpcId --query 'GroupId' --output text`
$ aws ec2 authorize-security-group-ingress --group-id $securityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre>

<h3 id="launching-your-instance">Launching your Instance</h3>

<p>All that&rsquo;s left to do is to create an SSH key pair and then launch an instance secured by this. Let&rsquo;s generate this key pair and store it locally with the correct permissions.</p>

<pre><code class="language-bash">$ aws ec2 create-key-pair --key-name my-key --query 'KeyMaterial' --output text &gt; ~/.ssh/my-key.pem
$ chmod 400 ~/.ssh/my-key.pem
</code></pre>

<p>We can now launch a single t2.micro instance based on the public AWS Ubuntu image.</p>

<pre><code class="language-bash">$ instanceId=`aws ec2 run-instances --image-id ami-9eaa1cf6 --count 1 --instance-type t2.micro --key-name my-key --security-group-ids $securityGroupId --subnet-id $subnetId --associate-public-ip-address --query 'Instances[0].InstanceId' --output text`
</code></pre>

<p>After a few minutes your instance should be up and running. You should now be able to obtain the url of your active instance and SSH into it.</p>

<pre><code class="language-bash">$ instanceUrl=`aws ec2 describe-instances --instance-ids $instanceId --query 'Reservations[0].Instances[0].PublicDnsName' --output text`
$ ssh -i ~/.ssh/my-key.pem ubuntu@$instanceUrl
</code></pre>

<p>And that&rsquo;s it. It&rsquo;s really not all that hard. There&rsquo;s just an awful lot of concepts that you need to get your head around which can make it a bit daunting at first. Be sure to check out the free <a href="http://www.amazon.com/gp/product/B007S33NT2/ref=cm_cr_ryp_prd_ttl_sol_0">Amazon Virtual Private Cloud User Guide</a> if you want to learn more about VPCs.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/finding_and_deleting_old_tags_in_a_github_repo/">
                Finding and deleting old tags in a Github repository
              </a>
            </h1>

            <span class="post-date">Jul 18, 2014</span>

            

            

<p>It&rsquo;s very easy for a Github repository to accumulate lots of tags over time. This onslaught of tags tends to be tolerated until it starts impacting git performance. It is at this point, when you have well in excess of tens of thousands of tags, that a call to action tends to be made. In this article, we&rsquo;ll look at two approaches to rid yourself of these old tags.</p>

<h3 id="the-cut-off-tag-approach">The cut-off tag approach</h3>

<p>This approach has us specify a cut-off tag. All tags that can trace their ancestry back to this cut-off tag will be allowed to remain. All others will get deleted. This is especially useful for when you have just merged a new feature, and now you want to delete all tags that were created before this merge. In this scenario, all you have to do is tag the merge commit and then use this as the cut-off tag.</p>

<p>The sequence of commands below deletes all tags that do not have the release-5 tag as an ancestor. Most of these commands are pretty self-explanatory, except for the one in the middle. The remainder of this section will focus on explaining this command.</p>

<pre><code class="language-bash"># fetch all tags from the remote
git fetch

# delete all tags on the remote that do not have the release-5 tag as an ancestor
comm -23 &lt;(git tag | sort) &lt;(git tag --contains release-5 | sort) | xargs git push --delete origin

# delete all local tags that are no longer present on the remote
git fetch --prune origin +refs/tags/*:refs/tags/*
</code></pre>

<p>The <a href="http://linux.die.net/man/1/comm">comm command</a> is used to <a href="http://www.unixcl.com/2009/08/linux-comm-command-brief-tutorial.html">compare two sorted files line by line</a>. Luckily, we can avoid having to create any actual files by relying on process substitution instead.</p>

<pre><code class="language-bash">comm -23 &lt;(command to act as file 1) &lt;(command to act as file 2) | xargs git push --delete origin
</code></pre>

<p>The <code>-23</code> flag tells <code>comm</code> to suppress any lines that are unique to file 2, as well as any lines that appear in both files. In other words, it causes <code>comm</code> to return just those lines that only appear in file 1. Looking back at our sequence of commands above, it should be clear that this will cause us to obtain all tags that do not have the release-5 tag as an ancestor. Piping this output to <code>xargs git push --delete origin</code> will then remove these tags from Github.</p>

<h3 id="the-cut-off-date-approach">The cut-off date approach</h3>

<p>While the cut-off tag approach works great in a lot of scenarios, sometimes you just want to delete all tags that were created before a given cut-off date instead. Unfortunately, git doesn&rsquo;t have any built-in functionality for accomplishing this. This is why we are going to make use of a Ruby script here.</p>

<pre><code class="language-ruby"># CUT_OFF_DATE needs to be of YYYY-MM-DD format
CUT_OFF_DATE = &quot;2015-05-10&quot;

def get_old_tags(cut_off_date)  
  `git log --tags --simplify-by-decoration --pretty=&quot;format:%ai %d&quot;`
  .split(&quot;\n&quot;)
  .each_with_object([]) do |line, old_tags|
    if line.include?(&quot;tag: &quot;)
      date = line[0..9]
      tags = line[28..-2].gsub(&quot;,&quot;, &quot;&quot;).concat(&quot; &quot;).scan(/tag: (.*?) /).flatten
      old_tags.concat(tags) if date &lt; cut_off_date
    end
  end
end

# fetch all tags from the remote
`git fetch`

# delete all tags on the remote that were created before the CUT_OFF_DATE
get_old_tags(CUT_OFF_DATE).each_slice(100) do |batch|
  system(&quot;git&quot;, &quot;push&quot;, &quot;--delete&quot;, &quot;origin&quot;, *batch)
end

# delete all local tags that are no longer present on the remote
`git fetch --prune origin +refs/tags/*:refs/tags/*`
</code></pre>

<p>This Ruby script should be pretty straightforward. The <code>get_old_tags</code> method might stand out a bit here. It can look pretty complex, but most of it is just string manipulation to get the date and tags of each line outputted by the <code>git log</code> command, and storing old tags in the <code>old_tags</code> array. Note how we invoke the <code>system</code> method with an array of arguments for those calls that require input. This protects us against possible shell injection.</p>

<p>Be careful, as running this exact script inside your repository will delete all tags created before 2015-05-10. Also, be sure to specify your cut-off date in YYYY-MM-DD format!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/adding_a_post_execution_hook_to_the_db_migrate_task/">
                Adding a post-execution hook to the db:migrate task
              </a>
            </h1>

            <span class="post-date">Jun 9, 2014</span>

            

            <p>A few days ago we discovered that our MySQL database&rsquo;s default character set and collation had been changed to the wrong values. Worse yet, it looked like this change had happened many months ago; something which we had been completely unaware of until now! In order to make sure this didn&rsquo;t happen again, we looked into adding a post-execution hook to the rails db:migrate task.</p>

<p>Our first attempt is shown below. Here, we append a post-execution hook to the existing db:migrate task by creating a new db:migrate task. In rake, when a task is defined twice, the behavior of the new task gets appended to the behavior of the old task. So even though the code below may give the impression of overwriting the rails db:migrate task, we are actually just appending a call to the <code>post_execution_hook</code> method to it.</p>

<pre><code class="language-ruby">namespace :db do
  def post_execution_hook
    puts 'This code gets run after the rails db:migrate task.'
    puts 'However, it only runs if the db:migrate task does not throw an exception.'
  end

  task :migrate do
    post_execution_hook
  end
end
</code></pre>

<p>However, the above example only runs the appended code if the original db:migrate task does not throw any exceptions. Luckily we can do better than that by taking a slightly different approach. Rather than appending code, we are going to have a go at prepending it instead.</p>

<pre><code class="language-ruby">namespace :db do
  def post_execution_hook
    puts 'This code gets run after the rails db:migrate task.'
    puts 'It will ALWAYS run.'
  end

  task :attach_hook do
    at_exit { post_execution_hook }
  end
end

Rake::Task['db:migrate'].enhance(['db:attach_hook'])
</code></pre>

<p>Here we make use of the <a href="http://ruby-doc.org/stdlib-2.0.0/libdoc/rake/rdoc/Rake/Task.html#method-i-enhance">enhance method</a> to add db:attach_hook as a prerequisite task to db:migrate. This means that calling db:migrate will now cause the db:attach_hook task to get executed before db:migrate gets run. The db:attach_hook task creates an <code>at_exit</code> hook that will trigger our post-execution code upon exit of the db:migrate task. Hence, our post-execution hook will now get called even when db:migrate raises an exception!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/installing_chromedriver/">
                Installing chromedriver
              </a>
            </h1>

            <span class="post-date">May 14, 2014</span>

            

            <p>Some time ago I needed to install <a href="https://sites.google.com/a/chromium.org/chromedriver/">chromedriver</a> on a ubuntu machine. While this wasn&rsquo;t too hard, I was nevertheless surprised by the number of open StackOverflow questions on this topic. So I decided to leave some notes for my future self.</p>

<p>First of all, let&rsquo;s install chromedriver.</p>

<pre><code class="language-bash">$ LATEST_RELEASE=$(curl http://chromedriver.storage.googleapis.com/LATEST_RELEASE)
$ wget http://chromedriver.storage.googleapis.com/$LATEST_RELEASE/chromedriver_linux64.zip
$ unzip chromedriver_linux64.zip
$ rm chromedriver_linux64.zip
$ sudo mv chromedriver /usr/local/bin
</code></pre>

<p>Let&rsquo;s see what happens when we try and run it.</p>

<pre><code class="language-bash">$ chromedriver

chromedriver: error while loading shared libraries: libgconf-2.so.4:
cannot open shared object file: No such file or directory
</code></pre>

<p>That&rsquo;s a bit unexpected. Luckily we can easily fix this.</p>

<pre><code class="language-bash">$ sudo apt-get install libgconf-2-4
</code></pre>

<p>Now that we have a functioning chromedriver, the only thing left to do is to install Chrome. After all, chromedriver can&rsquo;t function without Chrome.</p>

<pre><code class="language-bash">$ wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
$ sudo sh -c 'echo &quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google.list'
$ sudo apt-get update
$ sudo apt-get install google-chrome-stable
</code></pre>

<p>And that&rsquo;s it. You should be good to go now.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/programmatically_rotating_the_android_screen/">
                Programmatically rotating the Android screen
              </a>
            </h1>

            <span class="post-date">Mar 20, 2014</span>

            

            <p>A lot of digital ink has been spilled on this subject, so I figured it might be worth to briefly talk about this. You can either change the orientation through ADB or through an app. While the ADB approach is the easiest, it might not work on all devices or on all Android versions. For example, the <code>dumpsys</code> output of a Kindle Fire is different than that of a Samsung Galaxy S4, so you might need to tweak the grepping of the output.</p>

<pre><code class="language-bash"># get current orientation
adb shell dumpsys input | grep SurfaceOrientation | awk '{print $2}'

# change orientaton to portait
adb shell content insert --uri content://settings/system --bind name:s:accelerometer_rotation --bind value:i:0
adb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:0

# change orientation to landscape
adb shell content insert --uri content://settings/system --bind name:s:accelerometer_rotation --bind value:i:0
adb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:1
</code></pre>

<p>If you dont want to use ADB and prefer to change the orientation through an Android app instead, then you can just use these commands.</p>

<pre><code class="language-java">// get current orientation
final int orientation = myActivity.getResources().getConfiguration().orientation;

// change orientation to portrait
myActivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);

// change orientation to landscape
myActivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/programmatically_creating_android_touch_events/">
                Programmatically creating Android touch events
              </a>
            </h1>

            <span class="post-date">Mar 4, 2014</span>

            

            <p>Recent versions of Android have the <code>adb shell input touch</code> functionality to simulate touch events on an Android device or simulator. However, older Android versions (like 2.3) do not support this command. Luckily it is possible to recreate this functionality by running <code>adb shell getevent</code> to capture events as they are being generated. These events can then later be replayed using the <code>adb shell sendevent</code> command.</p>

<p>Running <code>adb shell getevent</code> when touching the screen might get you something like shown below. Notice how the output is in hexadecimal.</p>

<pre><code class="language-bash">/dev/input/event7: 0001 014a 00000001
/dev/input/event7: 0003 003a 00000001
/dev/input/event7: 0003 0035 000001ce
/dev/input/event7: 0003 0036 00000382
/dev/input/event7: 0000 0002 00000000
/dev/input/event7: 0000 0000 00000000
/dev/input/event7: 0001 014a 00000000
/dev/input/event7: 0003 003a 00000000
/dev/input/event7: 0003 0035 000001ce
/dev/input/event7: 0003 0036 00000382
/dev/input/event7: 0000 0002 00000000
/dev/input/event7: 0000 0000 00000000
</code></pre>

<p>However, the <code>adb shell sendevent</code> command expect all of its input to be in decimal. So if we wanted to replay the above events, we&rsquo;d need to do something like shown below. Note that 462 and 898 are the x and y coordinates of this particular touch event.</p>

<pre><code class="language-bash">adb shell sendevent /dev/input/event7: 1 330 1
adb shell sendevent /dev/input/event7: 3 58 1
adb shell sendevent /dev/input/event7: 3 53 462
adb shell sendevent /dev/input/event7: 3 54 898
adb shell sendevent /dev/input/event7: 0 2 0
adb shell sendevent /dev/input/event7: 0 0 0
adb shell sendevent /dev/input/event7: 1 330 0
adb shell sendevent /dev/input/event7: 3 58 0
adb shell sendevent /dev/input/event7: 3 53 462
adb shell sendevent /dev/input/event7: 3 54 898
adb shell sendevent /dev/input/event7: 0 2 0
adb shell sendevent /dev/input/event7: 0 0 0
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/some_lesser_known_github_api_functionality/">
                Some lesser known Github API functionality
              </a>
            </h1>

            <span class="post-date">Feb 8, 2014</span>

            

            

<p>One of our automation tools occasionally needs to interact with our Github repositories. Unfortunately, the current implementation of this tool leaves something to be desired as it requires cloning these repositories to local disk. Changes against these local repositories are then made on local branches, after which these branches get pushed to Github.</p>

<p>However, in order to save on disk space this tool will only ever create a single local copy of each repository. This makes it unsafe to run multiple instances of this tool as multiple instances simultaneously executing sequences of git commands against the same local repositories might lead to these commands inadvertently getting interpolated, thereby leaving the local repositories in an undefined state.</p>

<p>The solution to this complexity was to completely remove the need for local repositories and instead aim to have everything done through the wonderful Github API. This article is a reminder to myself about some API functionality that I found while looking into this.</p>

<h3 id="checking-if-a-branch-contains-a-commit">Checking if a branch contains a commit</h3>

<p>While the Github API does not have an explicit call to check whether a given commit is included in a branch, we can nevertheless use the <a href="https://developer.github.com/v3/repos/commits/#compare-two-commits">compare call</a> for just this purpose. This call takes two commits as input and returns a large JSON response of comparison data. We can use the <code>status</code> field of the response to ascertain if a given commit is behind or identical to the HEAD commit of a branch. If so, then the branch contains that commit.</p>

<p>We can use the <a href="https://github.com/octokit/octokit.rb">Ruby octokit gem</a> to implement this as follows.</p>

<pre><code class="language-ruby">require 'octokit'

class GithubClient &lt; Octokit::Client
  def branch_contains_sha?(repo, branch, sha)
    ['behind', 'identical'].include?(compare(repo, branch, sha).status)
  end
end
</code></pre>

<h3 id="creating-a-remote-branch-from-a-remote-commit">Creating a remote branch from a remote commit</h3>

<p>Sometimes you&rsquo;ll want to create a remote branch by branching from a remote commit. We can use the <a href="https://developer.github.com/v3/git/refs/#create-a-reference">create_reference call</a> to accomplish this. Note that the <code>ref</code> parameter of this call needs to be set to <code>refs/heads/#{branch}</code> when creating a remote branch.</p>

<pre><code class="language-ruby">require 'octokit'

class GithubClient &lt; Octokit::Client
  def create_branch_from_sha(repo, branch, sha)
    # create_ref internally transforms &quot;heads/#{branch}&quot; into &quot;refs/heads/#{branch}&quot;
    # as mentioned above, this is required by the Github API
    create_ref(repo, &quot;heads/#{branch}&quot;, sha)
  end
end
</code></pre>

<h3 id="setting-the-head-of-a-remote-branch-to-a-specific-remote-commit">Setting the HEAD of a remote branch to a specific remote commit</h3>

<p>You can even forcefully set the HEAD of a remote branch to a specific remote commit by using the <a href="https://developer.github.com/v3/git/refs/#update-a-reference">update_reference call</a>. As mentioned earlier, the <code>ref</code> parameter needs to be set to <code>refs/heads/#{branch}</code>. Be careful when using this functionality though as it essentially allows you to overwrite the history of a remote branch!</p>

<pre><code class="language-ruby">require 'octokit'

class GithubClient &lt; Octokit::Client
  def update_branch_to_sha(repo, branch, sha, force = true)
    # update_ref internally transforms &quot;heads/#{branch}&quot; into &quot;refs/heads/#{branch}&quot;
    # as mentioned earlier, this is required by the Github API
    update_ref(repo, &quot;heads/#{branch}&quot;, sha, force)
  end
end
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/the_amazing_bitwise_xor_operator/">
                The amazing bitwise XOR operator
              </a>
            </h1>

            <span class="post-date">Jan 12, 2014</span>

            

            

<p>One of my colleagues recently mentioned this interview question to me.</p>

<blockquote>
<p>Imagine there is an array which contains 2n+1 elements, n of which have exactly one duplicate. Can you find the one unique element in this array?</p>
</blockquote>

<p>This seemed simple enough and I quickly came up with the Ruby solution below.</p>

<pre><code class="language-ruby">&gt; array = [3, 5, 4, 5, 3]
# =&gt; [3, 5, 4, 5, 3]
&gt; count = array.each_with_object(Hash.new(0)) { |number, hash| hash[number] += 1 }
# =&gt; {3=&gt;2, 5=&gt;2, 4=&gt;1}
&gt; count.key(1)
# =&gt; 4
</code></pre>

<p>I thought that would be the end of it, but instead I was asked if I could see a way to solve the problem in a significantly more performant way using the XOR operator.</p>

<h3 id="xor-characteristics">XOR characteristics</h3>

<p>In order to solve this problem with the XOR operator, we first need to understand some of its characteristics. This operator obeys the following rules:</p>

<ul>
<li>commutativity: <code>A^B=B^A</code></li>
<li>associativity: <code>(A^B)^C=A^(B^C)</code></li>
<li>the identity element is 0: <code>A^0=A</code></li>
<li>each element is its own inverse: <code>A^A=0</code></li>
</ul>

<p>Now imagine an array with the elements <code>[3, 5, 4, 5, 3]</code>. Using the above rules, we can show that XORing all these elements will leave us with the array&rsquo;s unique element.</p>

<pre><code class="language-ruby">accum = 3 ^ 5 ^ 4 ^ 5 ^ 3
accum = 0 ^ 3 ^ 5 ^ 4 ^ 5 ^ 3    # 0 is the identity element
accum = 0 ^ 3 ^ 3 ^ 4 ^ 5 ^ 5    # commutativity and associativity rules
accum = 0 ^ 0 ^ 4 ^ 0            # A^A = 0
accum = 4                        # 0 is the identity element
</code></pre>

<p>Putting this approach in code would give us something like this.</p>

<pre><code class="language-ruby">&gt; array = [3, 5, 4, 5, 3]
# =&gt; [3, 5, 4, 5, 3]
&gt; accum = 0
# =&gt; 0
&gt; array.each { |number| accum = accum ^ number }
# =&gt; [3, 5, 4, 5, 3]
&gt; accum
# =&gt; 4
</code></pre>

<h3 id="benchmarks">Benchmarks</h3>

<p>Let&rsquo;s use Ruby&rsquo;s <code>Benchmark</code> module to do a comparison of both approaches.</p>

<pre><code class="language-ruby">require 'benchmark'

array = [-1]
1000000.times do |t|
  array &lt;&lt; t
  array &lt;&lt; t
end

Benchmark.measure do
  count = array.each_with_object(Hash.new(0)) { |number, hash| hash[number] += 1 }
  count.key(1)
end
# =&gt; #&lt;Benchmark::Tms:0x007f83fa0279e0 @label=&quot;&quot;, @real=0.83534, @cstime=0.0, @cutime=0.0, @stime=0.010000000000000009, @utime=0.8300000000000005, @total=0.8400000000000005&gt;

Benchmark.measure do
  accum = 0
  array.each { |number| accum = accum ^ number }
  accum
end
# =&gt; #&lt;Benchmark::Tms:0x007f83fa240ba0 @label=&quot;&quot;, @real=0.136726, @cstime=0.0, @cutime=0.0, @stime=0.0, @utime=0.13999999999999968, @total=0.13999999999999968&gt;
</code></pre>

<p>So there you have it. Given an array that contains two million elements, the XOR operator approach turns out to be more than 6 times faster than utilizing a hashmap. That&rsquo;s quite a nice performance improvement!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/a_visual_explanation_of_sql_joins/">
                A visual explanation of SQL joins
              </a>
            </h1>

            <span class="post-date">Nov 17, 2013</span>

            

            <p>I admit that I find myself going to <a href="http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/">this article</a> every time I need to write some joins. Hopefully putting it here will save me from always having to google it.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/check_the_order_of_your_rescue_from_handlers/">
                Check the order of your rescue_from handlers!
              </a>
            </h1>

            <span class="post-date">Nov 11, 2013</span>

            

            <p>Our <code>rescue_from</code> handlers used to be defined like shown below. This might look okay to you. At first glance everything looks fine, right?</p>

<pre><code class="language-ruby">class WidgetsController &lt; ActionController::Base
  rescue_from ActionController::RoutingError, :with =&gt; :render_404
  rescue_from Exception,                      :with =&gt; :render_500
end
</code></pre>

<p>Turns out it&rsquo;s not okay at all. Handlers are searched <a href="http://apidock.com/rails/ActiveSupport/Rescuable/ClassMethods/rescue_from">from bottom to top</a>. This means that they should always be defined in order of most generic to most specific. Or in other words, the above code is exactly the wrong thing to do. Instead, we need to write our handlers like shown here.</p>

<pre><code class="language-ruby">class WidgetsController &lt; ActionController::Base
  rescue_from Exception,                      :with =&gt; :render_500
  rescue_from ActionController::RoutingError, :with =&gt; :render_404
end
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/the_javascript_event_loop/">
                The javascript event loop
              </a>
            </h1>

            <span class="post-date">Nov 10, 2013</span>

            

            <p>Sometimes you come across an article that is so well written you can&rsquo;t do anything but link to it. So if you&rsquo;ve ever wondered why the javascript runtime is so good at asynchronous operations, then you should definitely give <a href="http://blog.carbonfive.com/2013/10/27/the-javascript-event-loop-explained/">this article</a> a read.</p>

<p>Some snippets:</p>

<blockquote>
<p>JavaScript runtimes contain a message queue which stores a list of messages to be processed and their associated callback functions. These messages are queued in response to external events (such as a mouse being clicked or receiving the response to an HTTP request) given a callback function has been provided. If, for example a user were to click a button and no callback function was provided  no message would have been enqueued.</p>

<p>In a loop, the queue is polled for the next message (each poll referred to as a tick) and when a message is encountered, the callback for that message is executed.</p>

<p>The calling of this callback function serves as the initial frame in the call stack, and due to JavaScript being single-threaded, further message polling and processing is halted pending the return of all calls on the stack.</p>
</blockquote>

<p>As well as:</p>

<blockquote>
<p>Using Web Workers enables you to offload an expensive operation to a separate thread of execution, freeing up the main thread to do other things. The worker includes a separate message queue, event loop, and memory space independent from the original thread that instantiated it. Communication between the worker and the main thread is done via message passing, which looks very much like the traditional, evented code-examples weve already seen.</p>
</blockquote>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/bug_hunting_with_git_bisect/">
                Bug hunting with git bisect
              </a>
            </h1>

            <span class="post-date">Nov 4, 2013</span>

            

            <p>Today I was looking into what I thought was going to be a simple bug. The problem seemed straightforward enough, so I did a quick grep of the codebase, found three pieces of code that looked like likely culprits, made some modifications, triggered the bug, and found that absolutely nothing had changed. Half an hour and a lot of additional digging later I was stumped. I had no idea what was going on.</p>

<p>It was at this point that I remembered <code>git bisect</code>. This git command asks you to specify two commits: one where things are working, and another one where things are broken. It then does a binary search across the range of commits in between these two. Each search step asks you whether the current commit contains broken code or not, after which it automatically selects the next commit for you. There&rsquo;s a great tutorial over <a href="http://webchick.net/node/99">here</a>.</p>

<pre><code class="language-bash">$ git bisect start
$ git bisect good rj6y4j3
$ git bisect bad 2q7f529
</code></pre>

<p>It took me all of five minutes to discover the source of the bug this way. I can safely say that it would have taken me ages to track down this particular bit of offending code as it was located in a custom bug fix for a popular third party library (I&rsquo;m looking at you <a href="https://github.com/getsentry/raven-ruby">Sentry</a>).</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/why_is_mysql_converting_my_nulls_to_blanks/">
                Why is MySQL converting my NULLs to blanks?
              </a>
            </h1>

            <span class="post-date">Nov 1, 2013</span>

            

            <p>A while ago I ran into an issue where some records were showing a blank value in a given column. This was a bit weird as a blank value had never been written to that column. After a bit of searching we found that we had a bug that had inadvertently been writing the occasional NULL value to that particular column though. So how did those NULLs get turned into blanks?</p>

<p>It turns out that MySQL can operate in different <a href="https://dev.mysql.com/doc/refman/5.0/en/sql-mode.html">server modes</a>. You can check your server mode by running one of the two commands below. Note that your server mode will be blank by default.</p>

<pre><code class="language-bash">SHOW GLOBAL VARIABLES where Variable_name = 'sql_mode';
SHOW SESSION VARIABLES where Variable_name = 'sql_mode'
</code></pre>

<p>Now that we know about server modes we can talk about <a href="https://dev.mysql.com/doc/refman/5.0/en/data-type-defaults.html">data type defaults</a>. Basically, each MySQL column has an implicit default value assigned to it. Under certain circumstances this default value might be used instead of the value you were expecting.</p>

<blockquote>
<p>As of MySQL 5.0.2, if a column definition includes no explicit DEFAULT value, MySQL determines the default value as follows:</p>

<p>If the column can take NULL as a value, the column is defined with an explicit DEFAULT NULL clause. This is the same as before 5.0.2.</p>

<p>If the column cannot take NULL as the value, MySQL defines the column with no explicit DEFAULT clause. Exception: If the column is defined as part of a PRIMARY KEY but not explicitly as NOT NULL, MySQL creates it as a NOT NULL column (because PRIMARY KEY columns must be NOT NULL), but also assigns it a DEFAULT clause using the implicit default value. To prevent this, include an explicit NOT NULL in the definition of any PRIMARY KEY column.</p>

<p>For data entry into a NOT NULL column that has no explicit DEFAULT clause, if an INSERT or REPLACE statement includes no value for the column, or an UPDATE statement sets the column to NULL, MySQL handles the column according to the SQL mode in effect at the time:</p>

<ul>
<li><em>If strict SQL mode is enabled, an error occurs for transactional tables and the statement is rolled back. For nontransactional tables, an error occurs, but if this happens for the second or subsequent row of a multiple-row statement, the preceding rows will have been inserted.</em></li>
<li><em>If strict mode is not enabled, MySQL sets the column to the implicit default value for the column data type.</em></li>
</ul>
</blockquote>

<p>We found that our code was sometimes writing NULLs to a NOT NULL column on a server that was not running in strict mode. This in turn caused our NULLs to silently get changed to blanks as this was the column default value. Mystery solved.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/using_environment_variables_in_migrations/">
                Using environment variables in migrations
              </a>
            </h1>

            <span class="post-date">Oct 29, 2013</span>

            

            <p>Recently we had to run a migration that was so slow we couldn&rsquo;t afford the downtime it would cause. In order to get around this, it was decided to put two code paths in the migration: one that was slow and thorough, and one that was quick but didn&rsquo;t perform any safety checks.</p>

<p>The first path would be run on a recent database dump, whereas the latter would be executed directly on the live database once the first had finished without error. This was a lot less crazy than it might sound as the particular table under modification had very infrequent changes.</p>

<p>It was decided to use environment variables to allow for easy switching between code paths. This is what the code ended up looking like.</p>

<pre><code class="language-ruby">class MyDangerousMigration &lt; ActiveRecord::Migration
  def change
    if ENV['skip_checks'] == 'true'
      # code without safety checks
    else
      # code with safety checks
    end
  end
end
</code></pre>

<p>This could then be run like so.</p>

<pre><code class="language-bash">skip_checks=true bundle exec rake db:migrate
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/getting_connection_information_with_lsof/">
                Getting connection information with lsof
              </a>
            </h1>

            <span class="post-date">Oct 21, 2013</span>

            

            

<p>The <a href="http://linux.die.net/man/8/lsof">lsof command</a> is one of those super useful commands for figuring out what connections are taking place on your machine. While the <code>lsof</code> command technically just lists open files, just about everything in linux (even sockets) is a file!</p>

<p>Some useful commands:</p>

<h4 id="list-all-network-connections">List all network connections</h4>

<pre><code class="language-bash">$ lsof -i

COMMAND     PID     USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
Spotify   36908 vaneyckt   53u  IPv4 0x2097c8deb175c0dd      0t0  TCP localhost:4381 (LISTEN)
Spotify   36908 vaneyckt   54u  IPv4 0x2097c8deab18027d      0t0  TCP localhost:4371 (LISTEN)
Spotify   36908 vaneyckt   71u  IPv4 0x2097c8deba747c1d      0t0  UDP *:57621
Spotify   36908 vaneyckt   72u  IPv4 0x2097c8deb18ef4cf      0t0  TCP *:57621 (LISTEN)
Spotify   36908 vaneyckt   77u  IPv4 0x2097c8deb993b255      0t0  UDP ip-192-168-0-101.ec2.internal:61009
Spotify   36908 vaneyckt   90u  IPv4 0x2097c8dea8c4a66d      0t0  TCP ip-192-168-0-101.ec2.internal:62432-&gt;lon3-accesspoint-a57.lon3.spotify.com:https (ESTABLISHED)
Spotify   36908 vaneyckt   91u  IPv4 0x2097c8de8d029f2d      0t0  UDP ip-192-168-0-101.ec2.internal:52706
</code></pre>

<h4 id="list-all-network-connections-on-port-4381">List all network connections on port 4381</h4>

<pre><code class="language-bash">$ lsof -i :4381

COMMAND   PID     USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
Spotify 36908 vaneyckt   53u  IPv4 0x2097c8deb175c0dd      0t0  TCP localhost:4381 (LISTEN)
</code></pre>

<h4 id="find-ports-listening-for-connections">Find ports listening for connections</h4>

<pre><code class="language-bash">$ lsof -i | grep -i LISTEN

Spotify   36908 vaneyckt   53u  IPv4 0x2097c8deb175c0dd      0t0  TCP localhost:4381 (LISTEN)
Spotify   36908 vaneyckt   54u  IPv4 0x2097c8deab18027d      0t0  TCP localhost:4371 (LISTEN)
Spotify   36908 vaneyckt   72u  IPv4 0x2097c8deb18ef4cf      0t0  TCP *:57621 (LISTEN)
</code></pre>

<h4 id="find-established-connections">Find established connections</h4>

<pre><code class="language-bash">$ lsof -i | grep -i ESTABLISHED

Spotify   36908 vaneyckt   90u  IPv4 0x2097c8dea8c4a66d      0t0  TCP ip-192-168-0-101.ec2.internal:62432-&gt;lon3-accesspoint-a57.lon3.spotify.com:https (ESTABLISHED)
</code></pre>

<h4 id="show-all-files-opened-by-a-given-process">Show all files opened by a given process</h4>

<pre><code class="language-bash">$ lsof -p 36908

COMMAND   PID     USER   FD     TYPE             DEVICE  SIZE/OFF     NODE NAME
Spotify 36908 vaneyckt   90u    IPv4 0x2097c8dea8c4a66d       0t0      TCP ip-192-168-0-101.ec2.internal:62432-&gt;lon3-accesspoint-a57.lon3.spotify.com:https (ESTABLISHED)
Spotify 36908 vaneyckt   91u    IPv4 0x2097c8de8d029f2d       0t0      UDP ip-192-168-0-101.ec2.internal:52706
Spotify 36908 vaneyckt   92u     REG                1,4   9389456 59387889 /Users/vaneyckt/Library/Caches/com.spotify.client/Data/4a/4a5a23cf1e9dc4210b3c801d57a899098dc12418.file
Spotify 36908 vaneyckt   93u     REG                1,4   8658944 58471210 /private/var/folders/xv/fjmwzr9x5mq_s7dchjq87hjm0000gn/T/.org.chromium.Chromium.6b0Vzp
Spotify 36908 vaneyckt   94u     REG                1,4    524656 54784499 /Users/vaneyckt/Library/Caches/com.spotify.client/Browser/index
Spotify 36908 vaneyckt   95u     REG                1,4     81920 54784500 /Users/vaneyckt/Library/Caches/com.spotify.client/Browser/data_0
Spotify 36908 vaneyckt   96u     REG                1,4    532480 54784501 /Users/vaneyckt/Library/Caches/com.spotify.client/Browser/data_1
Spotify 36908 vaneyckt   97u     REG                1,4   2105344 54784502 /Users/vaneyckt/Library/Caches/com.spotify.client/Browser/data_2
Spotify 36908 vaneyckt   98u     REG                1,4  12591104 54784503 /Users/vaneyckt/Library/Caches/com.spotify.client/Browser/data_3
Spotify 36908 vaneyckt   99r     REG                1,4    144580    28952 /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/Resources/HIToolbox.rsrc
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/carefully_converting_your_mysql_database_to_utf8/">
                Carefully converting your MySQL database to utf8
              </a>
            </h1>

            <span class="post-date">Oct 20, 2013</span>

            

            <p>Converting all the data in your database can be a nail-biting experience. As you can see from the code below we are doing our best to be super careful. We convert each table separately and before each conversion we store the table&rsquo;s column types and an MD5 hash of every row in the table (we were lucky enough to not have enormous tables). After converting the table we check that no column types or rows were changed. It goes without saying that we do a trial run on a database dump first.</p>

<pre><code class="language-ruby">require 'set'
require 'digest/md5'

CHARACTER_SET = 'utf8'
COLLATION = 'utf8_unicode_ci'

class ConvertAllTablesToUtf8 &lt; ActiveRecord::Migration
  def up
    ActiveRecord::Base.connection.tables.each do |table|
      ActiveRecord::Base.transaction do
        ActiveRecord::Base.connection.execute(&quot;LOCK TABLES #{table} WRITE&quot;)
          say &quot;starting work on table: #{table}&quot;

          model = table.classify.constantize
          say &quot;associated model: #{model}&quot;

          say 'storing column types information before converting table to unicode'
          column_types_before = model.columns_hash.each_with_object({}) do |(column_name, column_info), column_types_before|
            column_types_before[column_name] = [column_info.sql_type, column_info.type]
          end

          say 'storing set of table data hashes before converting table to unicode'
          table_data_before = Set.new
          model.find_each do |datum|
            table_data_before &lt;&lt; Digest::MD5.hexdigest(datum.inspect)
          end

          say 'converting table to unicode'
          execute(&quot;ALTER TABLE #{table} CONVERT TO CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)
          execute(&quot;ALTER TABLE #{table} DEFAULT CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)

          say 'getting column types information after conversion to unicode'
          column_types_after = model.columns_hash.each_with_object({}) do |(column_name, column_info), column_types_after|
            column_types_after[column_name] = [column_info.sql_type, column_info.type]
          end

          say 'getting set of table data hashes after conversion to unicode'
          table_data_after = Set.new
          model.find_each do |datum|
            table_data_after &lt;&lt; Digest::MD5.hexdigest(datum.inspect)
          end

          say &quot;checking that column types haven't changed&quot;
          if column_types_before != column_types_after
            raise &quot;Column types of the #{table} table have changed&quot;
          end

          say &quot;checking that data hasn't changed&quot;
          if table_data_before != table_data_after
            raise &quot;Data in the #{table} table has changed&quot;
          end
        ActiveRecord::Base.connection.execute('UNLOCK TABLES')
      end
    end

    execute(&quot;ALTER DATABASE #{ActiveRecord::Base.connection.current_database} DEFAULT CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)
  end

  def down
    raise ActiveRecord::IrreversibleMigration
  end
end
</code></pre>

<p>Note how we lock each table before converting it. If we didn&rsquo;t lock it then new data could be written to the table while we are busy storing MD5 hashes of the rows in preparation for the actual conversion. This, in turn, would cause our migration to complain that new data was present after the conversion had taken place.</p>

<p>We also wrap each table conversion inside a transaction. I&rsquo;ve talked before about <a href="http://vaneyckt.io/posts/rails_migrations_and_the_dangers_of_implicit_commits/">how converting a table will cause an implicit commit</a>, meaning that a rollback won&rsquo;t undo any of the changes made by the conversion. So why have a transaction here then? Imagine that an exception were to be raised during our migration. In that case we want to ensure our table lock gets dropped as soon as possible. The transaction guarantees this behavior.</p>

<p>Also, if we weren&rsquo;t so paranoid about checking the before and after data as part of our migration, we could simplify this code quite a bit.</p>

<pre><code class="language-ruby">CHARACTER_SET = 'utf8'
COLLATION = 'utf8_unicode_ci'

class ConvertAllTablesToUtf8 &lt; ActiveRecord::Migration
  def up
    ActiveRecord::Base.connection.tables.each do |table|
      say 'converting table to unicode'
      execute(&quot;ALTER TABLE #{table} CONVERT TO CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)
      execute(&quot;ALTER TABLE #{table} DEFAULT CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)
    end

    execute(&quot;ALTER DATABASE #{ActiveRecord::Base.connection.current_database} DEFAULT CHARACTER SET #{CHARACTER_SET} COLLATE #{COLLATION}&quot;)
  end

  def down
    raise ActiveRecord::IrreversibleMigration
  end
end
</code></pre>

<p>Notice how we can drop the lock as the <code>ALTER TABLE</code> command <a href="https://dev.mysql.com/doc/refman/5.1/en/alter-table.html">will prevent all writes to the table while simultaneously allowing all reads</a>.</p>

<blockquote>
<p>In most cases, ALTER TABLE makes a temporary copy of the original table. MySQL waits for other operations that are modifying the table, then proceeds. It incorporates the alteration into the copy, deletes the original table, and renames the new one. While ALTER TABLE is executing, the original table is readable by other sessions. Updates and writes to the table that begin after the ALTER TABLE operation begins are stalled until the new table is ready, then are automatically redirected to the new table without any failed updates. The temporary copy of the original table is created in the database directory of the new table. This can differ from the database directory of the original table for ALTER TABLE operations that rename the table to a different database.</p>
</blockquote>

<p>Furthermore, since we now no longer have a lock on our table we can also drop the transaction. This gives us the much-simplified code shown above.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/character_set_vs_collation/">
                Character set vs collation
              </a>
            </h1>

            <span class="post-date">Oct 19, 2013</span>

            

            <p>There&rsquo;s a surprising amount of confusion about the difference between these two terms. The best explanation I&rsquo;ve found is <a href="http://stackoverflow.com/questions/341273/what-does-character-set-and-collation-mean-exactly/341481#341481">here</a>.</p>

<blockquote>
<p>A character set is a subset of all written glyphs. A character encoding specifies how those characters are mapped to numeric values. Some character encodings, like UTF-8 and UTF-16, can encode any character in the Universal Character Set. Others, like US-ASCII or ISO-8859-1 can only encode a small subset, since they use 7 and 8 bits per character, respectively. Because many standards specify both a character set and a character encoding, the term &ldquo;character set&rdquo; is often substituted freely for &ldquo;character encoding&rdquo;.</p>

<p>A collation comprises rules that specify how characters can be compared for sorting. Collations rules can be locale-specific: the proper order of two characters varies from language to language.</p>

<p>Choosing a character set and collation comes down to whether your application is internationalized or not. If not, what locale are you targeting?</p>

<p>In order to choose what character set you want to support, you have to consider your application. If you are storing user-supplied input, it might be hard to foresee all the locales in which your software will eventually be used. To support them all, it might be best to support the UCS (Unicode) from the start. However, there is a cost to this; many western European characters will now require two bytes of storage per character instead of one.</p>

<p>Choosing the right collation can help performance if your database uses the collation to create an index, and later uses that index to provide sorted results. However, since collation rules are often locale-specific, that index will be worthless if you need to sort results according to the rules of another locale.</p>
</blockquote>

<p>The only thing I&rsquo;d like to add is that some collations are more cpu intensive than others. For example, <code>utf8_general_ci</code> treats , , and  as being equal to A when doing comparisons. This is in contrast to <code>utf8_unicode_ci</code> which uses about 10% more cpu, but differentiates between these characters.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/mysql_write_locks_also_prevent_reads/">
                MySQL write locks also prevent reads
              </a>
            </h1>

            <span class="post-date">Oct 17, 2013</span>

            

            <p>Locking a table with</p>

<pre><code class="language-ruby">table_name = 'widgets'
ActiveRecord::Base.connection.execute(&quot;LOCK TABLES #{table_name} WRITE&quot;)
</code></pre>

<p>ensures that <a href="http://dev.mysql.com/doc/refman/5.0/en/lock-tables.html">only the current connection can access that table</a>. Other connections cannot even read from this table while it is locked!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/rails_migrations_and_the_dangers_of_implicit_commits/">
                Rails migrations and the dangers of implicit commits
              </a>
            </h1>

            <span class="post-date">Oct 16, 2013</span>

            

            <p>I recently came across the migration below. At first sight it looks like everything is okay, but there is actually a very dangerous assumption being made here.</p>

<pre><code class="language-ruby"># migration to convert table to utf8
class ConvertWidgetsTableToUtf8Unicode &lt; ActiveRecord::Migration
  def up
    ActiveRecord::Base.transaction do
      table_name = 'widgets'
      say &quot;converting #{table_name} table to utf8_unicode_ci&quot;

      execute(&quot;ALTER TABLE #{table_name} CONVERT TO CHARACTER SET utf8 COLLATE utf8_unicode_ci&quot;)
      execute(&quot;ALTER TABLE #{table_name} DEFAULT CHARACTER SET utf8 COLLATE utf8_unicode_ci&quot;)
    end
  end
end
</code></pre>

<p>Notice how the utf8 conversion code is wrapped inside a transaction. The assumption here is that if something goes wrong the transaction will trigger a rollback. However, an <code>ALTER TABLE</code> command in MySQL causes an <a href="http://dev.mysql.com/doc/refman/5.0/en/implicit-commit.html">implicit commit</a>. This means that the rollback will not undo any changes introduced by the <code>ALTER TABLE</code> command!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/iterating_over_a_hash_containing_arrays/">
                Iterating over a hash containing arrays
              </a>
            </h1>

            <span class="post-date">Oct 15, 2013</span>

            

            <p>Last week I was implementing some auditing functionality in a rails app. At some point I was writing a page that would display how the attributes of a given ActiveRecord object had been changed. One of my colleagues spotted this and pointed out the following neat bit of syntactic sugar in Ruby.</p>

<pre><code class="language-ruby">changes = {:attribute_a =&gt; [1, 2], :attribute_b =&gt; [3, 4]}

changes.each do |attribute, (before, after)|
  puts &quot;#{attribute}: #{before} - #{after}&quot;
end
</code></pre>

<p>I later learned you can even do things like this.</p>

<pre><code class="language-ruby">data = {:foo =&gt; [[1, 2], 3]}

data.each do |key, ((a, b), c)|
  puts &quot;#{key}: #{a} - #{b} - #{c}&quot;
end
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/uri_js_and_url_manipulation_in_rails/">
                URI.js and URL manipulation in rails
              </a>
            </h1>

            <span class="post-date">Oct 13, 2013</span>

            

            <p>Manipulating urls in javascript often ends up being an exercise in string interpolation. This rarely produces good looking code. Recently we&rsquo;ve started enforcing the use of the <a href="https://medialize.github.io/URI.js/">URI.js library</a> to combat this.</p>

<p>Our new approach has us embed any necessary urls in hidden input fields on the web page in question. Rather than hardcoding these urls, we use the named route functionality offered by rails as this provides more flexibility. When the page gets rendered, these named routes are converted to actual urls through ERB templating. The embedded urls can then be fetched by javascript code and manipulated with URI.js.</p>

<p>It&rsquo;s no silver bullet, but the resulting code is a lot more readable.</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/the_css_important_keyword/">
                The css !important keyword
              </a>
            </h1>

            <span class="post-date">Oct 12, 2013</span>

            

            <p>Today I learned about the css !important keyword. I was trying to change the way code snippets (gists) were being displayed on a site, but found my css rules being ignored.</p>

<p>As it turned out, the javascript snippets used for embedding gists were adding an additional css stylesheet to the page. Since this stylesheet was getting added after my own stylesheet, its rules had priority over my own. The solution was to add <code>!important</code> to my own rules.</p>

<pre><code class="language-css">.gist-data {
  border-bottom: 1px !important;
}
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/finding_models_from_strings_with_rails/">
                Finding models from strings with rails
              </a>
            </h1>

            <span class="post-date">Oct 11, 2013</span>

            

            <p>Imagine you have a Widget model that stores data in a table &lsquo;widgets&rsquo;. At some point in your rails app you find yourself being given a string &lsquo;Widget&rsquo; and are asked to find the Widget model. This can be done like shown here.</p>

<pre><code class="language-ruby">str = 'Widget'
model = str.constantize
</code></pre>

<p>However, things get a bit harder when you have multiple Widget model subclasses (Widget::A, Widget::B), all of which are stored in the widgets table. This time around you&rsquo;re given the string &lsquo;Widget::A&rsquo; and are asked to get the Widget model.</p>

<p>In order to solve this we&rsquo;ll need to ask the Widget::A model to give us its table name. If you&rsquo;re following rails conventions you can then in turn use the table name to get the model you need.</p>

<pre><code class="language-ruby">str = 'Widget'
model = str.constantize.table_name.classify.constantize
</code></pre>

<p>Note that the above will only work if you&rsquo;ve followed rails naming conventions though :).</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/retrieving_data_in_a_time_range_with_rails/">
                Retrieving data in a time range with rails
              </a>
            </h1>

            <span class="post-date">Oct 10, 2013</span>

            

            <p>I&rsquo;m writing this mostly as a reminder to myself, since I keep forgetting this :)</p>

<p>Instead of:</p>

<pre><code class="language-ruby">widgets = Widget.where(&quot;? &lt;= created_at AND created_at &lt;= ?&quot;, time_from, time_to)
</code></pre>

<p>do this:</p>

<pre><code class="language-ruby">widgets = Widget.where(:created_at =&gt; time_from .. time_to)
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/get_vs_post/">
                GET vs POST
              </a>
            </h1>

            <span class="post-date">Oct 9, 2013</span>

            

            

<p>Today I was looking into why a particular GET request was failing on IE. As it turned out this was due to IE not appreciating long query strings. While going through our nginx logs, we also found nginx had a default query string limit that was being hit sporadically by some other customers as well. The solution in both cases was to move the affected calls from GET to POST.</p>

<p>The above problem prompted me to take a closer look at the differences between GET and POST requests. You probably use these all the time, but do you know how each of them functions?</p>

<h4 id="get-requests">GET requests</h4>

<ul>
<li>can be bookmarked</li>
<li>can be cached for faster response time on subsequent request</li>
<li>request is stored in browser history</li>
<li>uses query strings to send data. There is a limit to the allowable length of a query string.</li>
<li>have their url and query strings stored in plaintext in server logs. This is why you should never send passwords over GET requests!</li>
<li>use these for actions that retrieve data. For example, you don&rsquo;t want to use GET requests for posting comments on your blog. Otherwise an attacker could copy a url that posts a specific comment and put it on twitter. Every time someone were to click this link, a comment would now be posted on your blog.</li>
</ul>

<h4 id="post-requests">POST requests</h4>

<ul>
<li>cannot be bookmarked</li>
<li>cannot be cached</li>
<li>request will not be stored in browser history</li>
<li>uses POST body to send data. There is no limit to the amount of data sent due to the multipart content-type spreading your data across multiple messages when necessary.</li>
<li>have their url stored in plaintext in server logs. The data itself will not be logged though.</li>
<li>use these for actions that alter data</li>
</ul>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/the_dig_command/">
                The dig command
              </a>
            </h1>

            <span class="post-date">Oct 8, 2013</span>

            

            <p>Today I learned of the existence of the <a href="http://linux.die.net/man/1/dig">dig command</a>. A very useful little tool for DNS lookups. Here&rsquo;s an example of it in action.</p>

<pre><code class="language-bash">$ dig www.google.com

; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; www.google.com
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 4868
;; flags: qr rd ra; QUERY: 1, ANSWER: 6, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;www.google.com.			IN	A

;; ANSWER SECTION:
www.google.com.		72	IN	A	74.125.24.105
www.google.com.		72	IN	A	74.125.24.103
www.google.com.		72	IN	A	74.125.24.104
www.google.com.		72	IN	A	74.125.24.99
www.google.com.		72	IN	A	74.125.24.147
www.google.com.		72	IN	A	74.125.24.106

;; Query time: 11 msec
;; SERVER: 192.168.0.1#53(192.168.0.1)
;; WHEN: Sat Aug 29 13:38:48 2015
;; MSG SIZE  rcvd: 128
</code></pre>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/profiling_rails_assets_precompilation/">
                Profiling rails assets precompilation
              </a>
            </h1>

            <span class="post-date">Sep 1, 2013</span>

            

            <p>Assets precompilation on rails can take a fair bit of time. This is especially annoying in scenarios where you want to deploy your app multiple times a day. Let&rsquo;s see if we can come up with a way to actually figure out where all this time is being spent. Also, while I will be focusing on rails 3.2 in this post, the general principle should be easy enough to apply to other rails versions.</p>

<p>Our first call of action is finding the assets precompilation logic. A bit of digging will turn up the <a href="https://github.com/rails/rails/blob/3-2-stable/actionpack/lib/sprockets/assets.rake">assets.rake file</a> for rails 3.2. The relevant code starts on lines 59-67 and from there on out invokes methods throughout the entire file.</p>

<pre><code class="language-ruby"># lines 59-67 of assets.rake
task :all do
  Rake::Task[&quot;assets:precompile:primary&quot;].invoke
  # We need to reinvoke in order to run the secondary digestless
  # asset compilation run - a fresh Sprockets environment is
  # required in order to compile digestless assets as the
  # environment has already cached the assets on the primary
  # run.
  if Rails.application.config.assets.digest
    ruby_rake_task(&quot;assets:precompile:nondigest&quot;, false)
  end
end
</code></pre>

<p>When we follow the calls made by the code above we can see that the actual compilation takes place on lines 50-56 of assets.rake and is done by the compile method of the <a href="https://github.com/rails/rails/blob/3-2-stable/actionpack/lib/sprockets/static_compiler.rb">Sprockets::StaticCompiler class</a>.</p>

<pre><code class="language-ruby"># compile method of Sprockets::StaticCompiler class
def compile
  manifest = {}
  env.each_logical_path(paths) do |logical_path|
    if asset = env.find_asset(logical_path)
      digest_path = write_asset(asset)
      manifest[asset.logical_path] = digest_path
      manifest[aliased_path_for(asset.logical_path)] = digest_path
    end
  end
  write_manifest(manifest) if @manifest
end
</code></pre>

<p>Now that we know which code does the compiling, we can think of two ways to add some profiling to this. We could checkout the rails repo from Github, modify it locally and point our Gemfile to our modified local version of rails. Or, we could create a new rake task and monkey patch the compile method of the Sprockets::StaticCompiler class. We&rsquo;ll go with the second option here as it is the more straightforward to implement.</p>

<p>We&rsquo;ll create a new rake file in the /lib/tasks folder of our rails app and name it <code>profile_assets_precompilation.rake</code>. We then copy the contents of assets.rake into it, and wrap this code inside a new &lsquo;profile&rsquo; namespace so as to avoid conflicts. At the top of this file we&rsquo;ll also add our monkey patched compile method so as to make it output profiling info. The resulting file should look like shown below.</p>

<pre><code class="language-ruby">namespace :profile do
  # monkey patch the compile method to output compilation times
  module Sprockets
    class StaticCompiler
      def compile
        manifest = {}
        env.each_logical_path(paths) do |logical_path|
          start_time = Time.now.to_f

          if asset = env.find_asset(logical_path)
            digest_path = write_asset(asset)
            manifest[asset.logical_path] = digest_path
            manifest[aliased_path_for(asset.logical_path)] = digest_path
          end

          # our profiling code
          duration = Time.now.to_f - start_time
          puts &quot;#{logical_path} - #{duration.round(3)} seconds&quot;
        end
        write_manifest(manifest) if @manifest
      end
    end
  end

  # contents of assets.rake
  namespace :assets do
    def ruby_rake_task(task, fork = true)
      env    = ENV['RAILS_ENV'] || 'production'
      groups = ENV['RAILS_GROUPS'] || 'assets'
      args   = [$0, task,&quot;RAILS_ENV=#{env}&quot;,&quot;RAILS_GROUPS=#{groups}&quot;]
      args &lt;&lt; &quot;--trace&quot; if Rake.application.options.trace
      if $0 =~ /rake\.bat\Z/i
        Kernel.exec $0, *args
      else
        fork ? ruby(*args) : Kernel.exec(FileUtils::RUBY, *args)
      end
    end

    # We are currently running with no explicit bundler group
    # and/or no explicit environment - we have to reinvoke rake to
    # execute this task.
    def invoke_or_reboot_rake_task(task)
      if ENV['RAILS_GROUPS'].to_s.empty? || ENV['RAILS_ENV'].to_s.empty?
        ruby_rake_task task
      else
        Rake::Task[task].invoke
      end
    end

    desc &quot;Compile all the assets named in config.assets.precompile&quot;
    task :precompile do
      invoke_or_reboot_rake_task &quot;assets:precompile:all&quot;
    end

    namespace :precompile do
      def internal_precompile(digest=nil)
        unless Rails.application.config.assets.enabled
          warn &quot;Cannot precompile assets if sprockets is disabled. Please set config.assets.enabled to true&quot;
          exit
        end

        # Ensure that action view is loaded and the appropriate
        # sprockets hooks get executed
        _ = ActionView::Base

        config = Rails.application.config
        config.assets.compile = true
        config.assets.digest  = digest unless digest.nil?
        config.assets.digests = {}

        env      = Rails.application.assets
        target   = File.join(Rails.public_path, config.assets.prefix)
        compiler = Sprockets::StaticCompiler.new(env,
                                                 target,
                                                 config.assets.precompile,
                                                 :manifest_path =&gt; config.assets.manifest,
                                                 :digest =&gt; config.assets.digest,
                                                 :manifest =&gt; digest.nil?)
        compiler.compile
      end

      task :all do
        Rake::Task[&quot;assets:precompile:primary&quot;].invoke
        # We need to reinvoke in order to run the secondary digestless
        # asset compilation run - a fresh Sprockets environment is
        # required in order to compile digestless assets as the
        # environment has already cached the assets on the primary
        # run.
        ruby_rake_task(&quot;assets:precompile:nondigest&quot;, false) if Rails.application.config.assets.digest
      end

      task :primary =&gt; [&quot;assets:environment&quot;, &quot;tmp:cache:clear&quot;] do
        internal_precompile
      end

      task :nondigest =&gt; [&quot;assets:environment&quot;, &quot;tmp:cache:clear&quot;] do
        internal_precompile(false)
      end
    end

    desc &quot;Remove compiled assets&quot;
    task :clean do
      invoke_or_reboot_rake_task &quot;assets:clean:all&quot;
    end

    namespace :clean do
      task :all =&gt; [&quot;assets:environment&quot;, &quot;tmp:cache:clear&quot;] do
        config = Rails.application.config
        public_asset_path = File.join(Rails.public_path, config.assets.prefix)
        rm_rf public_asset_path, :secure =&gt; true
      end
    end

    task :environment do
      if Rails.application.config.assets.initialize_on_precompile
        Rake::Task[&quot;environment&quot;].invoke
      else
        Rails.application.initialize!(:assets)
        Sprockets::Bootstrap.new(Rails.application).run
      end
    end
  end
end
</code></pre>

<p>Now we can run <code>bundle exec rake profile:assets:precompile</code> to precompile our assets while outputting profiling info. Hopefully we can now finally figure out why this is always taking so long :).</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/regarding_if_statement_scope_in_ruby/">
                Regarding if statement scope in Ruby
              </a>
            </h1>

            <span class="post-date">Aug 31, 2013</span>

            

            <p>I recently learned that <code>if</code> statements in Ruby do not introduce scope. This means that you can write code like shown below and it&rsquo;ll work fine.</p>

<pre><code class="language-ruby"># perfectly valid Ruby code
if true
  foo = 5
end

puts foo
</code></pre>

<p>At first this seemed a bit weird to me. It wasn&rsquo;t until I read <a href="http://programmers.stackexchange.com/questions/58900/why-if-statements-do-not-introduce-scope-in-ruby-1-9">this</a> that I realized Ruby was even more versatile than I had first thought. As it turns out, it is this somewhat unconventional scoping rule that allows us to conditionally replace methods.</p>

<pre><code class="language-ruby">if foo == 5
  def some_method
    # do something
  end
else
  def some_method
    # do something else
  end
end
</code></pre>

<p>As well as conditionally modify implementations.</p>

<pre><code class="language-ruby">if foo == 5
  class someClass
    # ...
  end
else
  module someModule
    # ...
  end
end
</code></pre>

<p>And that&rsquo;s amazing!</p>

            <hr>
          </div>
        
      
        
          <div class="post">
            <h1 class="post-title">
              <a href="https://vaneyckt.io/posts/ec2_instance_cost_comparison/">
                EC2 instance cost comparison
              </a>
            </h1>

            <span class="post-date">Aug 11, 2013</span>

            

            <p>Amazon&rsquo;s pricing scheme for its ec2 instances never struck me as particularly transparent. Until recently some of my DevOps colleagues even estimated cost by cross-referencing <a href="https://aws.amazon.com/ec2/instance-types">instance details</a> with <a href="http://aws.amazon.com/ec2/pricing">pricing information</a>. While this approach gives reasonable results for finding the cost of a given instance type, it doesn&rsquo;t lend itself very well to comparing prices across a range of different types.</p>

<p>When talking to an ex-colleague of mine about the hardships encountered for such a common task, he pointed me to <a href="http://www.ec2instances.info">this absolutely brilliant page</a>. It&rsquo;s so unbelievably simple and well thought-out that I can&rsquo;t help getting ever so slightly annoyed with whomever is in charge of communicating Amazon&rsquo;s pricing structure and the subpar job they are doing.</p>

            <hr>
          </div>
        
      
    </div>
  </div>

  
  <a href="https://imgur.com/VZpijF9">
    <img src="https://i.imgur.com/VZpijF9.png" title="Viewer Count Tracking Pixel"/>
  </a>
</body>
</html>
